


<!DOCTYPE html>
<html lang="en">
<head>
    
    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script> <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>


    <!-- Library libs_ext -->
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>
    <link rel="icon" href="/favicon.svg"/>
    <title>COLM 2024: Accepted Papers</title>
    
</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/logo.jpg"
                    height="auto"
            width="180px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
              
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="index.html" 
                      
                      >Home</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown1">
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="dates.html" 
                      
                      >Dates</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown2">
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link dropdown-toggle" href="#" 
                      
                      id="navbarDropdown3" role="button" data-toggle="dropdown"
                      
                      >Program</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown3">
                      
                      <a class="dropdown-item" href="Keynotes.html">Keynote Speakers</a>
                      
                      <a class="dropdown-item" href="AcceeptedPapers.html">Accepted Papers</a>
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="cfp.html" 
                      
                      >Call for Papers</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown4">
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="AreaChairs.html" 
                      
                      >Area Chairs</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown5">
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link dropdown-toggle" href="#" 
                      
                      id="navbarDropdown6" role="button" data-toggle="dropdown"
                      
                      >Guides</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown6">
                      
                      <a class="dropdown-item" href="CoC.html">Code of Conduct</a>
                      
                      <a class="dropdown-item" href="CoE.html">Code of Ethics</a>
                      
                      <a class="dropdown-item" href="ReviewGuide.html">Review Guidelines</a>
                      
                      <a class="dropdown-item" href="AuthorGuide.html">Author Guide</a>
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="sponsors.html" 
                      
                      >Sponsors</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown7">
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="faq.html" 
                      
                      >FAQ</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown8">
                      
                    </div>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        



<h2>
    Accepted Papers
</h2>

<br />


<p>A Survey on Deep Learning for Theorem Proving <br/> <em>Jialiang Sun, Kaiyu Yang, Logan Murphy, Qidong Su, Xian Zhang, Xujie Si, Zenan Li, Zhaoyu Li</em></p>
 

<p>Towards Measuring the Representation of Subjective Global Opinions in Language Models <br/> <em>Alex Tamkin, Amanda Askell, Anton Bakhtin, Carol Chen, Danny Hernandez, Deep Ganguli, Esin DURMUS, Jack Clark, Janel Thamkul, Jared Kaplan, Karina Nguyen, Liane Lovitt, Nicholas Joseph, Nicholas Schiefer, Orowa Sikder, Sam McCandlish, Thomas Liao, Zac Hatfield-Dodds</em></p>
 

<p>Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM <br/> <em>Chunqiu Steven Xia, LINGMING ZHANG, Yinlin Deng</em></p>
 

<p>Transformer Circuit Evaluation Metrics Are Not Robust <br/> <em>Bilal Chughtai, Joseph Miller, William Saunders</em></p>
 

<p>Long-form Answers to Visual Questions Asked by Blind and Low Vision People <br/> <em>Amy Pavel, Chongyan Chen, Danna Gurari, Eunsol Choi, Fangyuan Xu, Hansika Murugu, Mina Huh, Yi-Hao Peng</em></p>
 

<p>Locating and Editing Factual Associations in Mamba <br/> <em>Arnab Sen Sharma, David Atkinson, David Bau</em></p>
 

<p>Does Incomplete Syntax Influence Korean Language Model? Focusing on Word Order and Case Markers <br/> <em>Ho-Jin Choi, Jong Myoung Kim, Sangkeun Jung, Yong-Jin Han, Young-Jun Lee</em></p>
 

<p>LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play <br/> <em>Li-Chun Lu, Shao-Hua Sun, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-yi Lee</em></p>
 

<p>Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided <br/> <em>Allen Zheng, Desmond Ong, Hongli Zhan, Jina Suh, Junyi Jessy Li, Yoon Kyung Lee</em></p>
 

<p>STEMA: Scalable Toolkit for Efficient Model Alignment <br/> <em>Ali Taghibakhshi, Ashwath Aithal, Daniel Egert, Gerald Shen, Jimmy J. Zhang, Markel Sanz Ausin, Oleksii Kuchaiev, Olivier Delalleau, Sahil Jain, Shengyang Sun, Yi Dong, Zhilin Wang, Jiaqi Zeng</em></p>
 

<p>Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language? <br/> <em>Alireza Sahebi, Doratossadat Dastgheib, Ehsaneddin Asgari, Marzia Nouri, Mohammad Hossein Rohban, Mohammad Vali Sanian, Omid Ghahroodi, Mahdieh Soleymani Baghshah</em></p>
 

<p>How Susceptible are LLMs to Influence in Prompts? <br/> <em>Jannis Bulian, Sotiris Anagnostidis</em></p>
 

<p>PairEval: Open-domain Dialogue Evaluation Metric with Pairwise Comparisons <br/> <em>ChaeHun Park, Dohyun Lee, Jaegul Choo, Minseok Choi</em></p>
 

<p>HGRN2: Gated Linear RNNs with State Expansion <br/> <em>Dong Li, Songlin Yang, Weigao Sun, Weixuan Sun, Xuyang Shen, Yiran Zhong, Zhen Qin</em></p>
 

<p>Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts <br/> <em>Alexander Rubinstein, Elisa Nguyen, Evgenii Kortukov, Seong Joon Oh</em></p>
 

<p>Investigating Instruction Tuning Large Language Models on Graphs <br/> <em>Bo-Wei Huang, Bowen Jin, Jiawei Han, Kerui Zhu, Kevin Chang, Ming Zhong, Shou-De Lin, Yizhu Jiao</em></p>
 

<p>FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers <br/> <em>Joshua Nathaniel Williams, J Zico Kolter</em></p>
 

<p>CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization <br/> <em>Bodhisattwa Prasad Majumder, Chris Callison-Burch, Li Zhang, Niket Tandon, Oyvind Tafjord, Peter Clark, Peter Jansen, Bhavana Dalvi Mishra</em></p>
 

<p>Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game <br/> <em>Silin Du, Xiaowei Zhang</em></p>
 

<p>Factual and Tailored Recommendation Endorsements using Language Models and Reinforcement Learning <br/> <em>ChihWei Hsu, Craig Boutilier, Jihwan Jeong, Yinlam Chow, Guy Tennenholtz, Mohammad Ghavamzadeh</em></p>
 

<p>How Well Do LLMs Identify Cultural Unity in Diversity? <br/> <em>Jialin Li, Junli Wang, Ming Jiang, Junjie Hu</em></p>
 

<p>Guiding Language Model Math Reasoning with Planning Tokens <br/> <em>Lucas Caccia, Oleksiy Ostapenko, Xinyi Wang, Alessandro Sordoni, William Yang Wang, Xingdi Yuan</em></p>
 

<p>Dated Data: Tracing Knowledge Cutoffs in Large Language Models <br/> <em>Daniel Khashabi, Dawn Lawrie, Jeffrey Cheng, Marc Marone, Orion Weller, Benjamin Van Durme</em></p>
 

<p>Large Language Model is not a (Multilingual) Compositional Relation Reasoner <br/> <em>Jinman Zhao, Xueyan Zhang</em></p>
 

<p>Instruction Mining: Instruction Data Selection for Tuning Large Language Models <br/> <em>Chi Wang, Lichao Sun, Yanbin Kang, Yihan Cao</em></p>
 

<p>Does your data spark joy? Performance gains from domain upsampling at the end of training <br/> <em>Cody Blakeney, Jonathan Frankle, Mansheej Paul, Sean Owen, Brett W. Larsen</em></p>
 

<p>Predicting Emergent Capabilities by Finetuning <br/> <em>Charlie Victor Snell, Dan Klein, Eric Wallace, Sergey Levine</em></p>
 

<p>Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data <br/> <em>Dana Alon, Donald Metzler, Tim Baumgärtner, Yang Gao</em></p>
 

<p>CATS: Context-Aware Thresholding for Sparsity in Large Language Models <br/> <em>Donghyun Lee, Genghan Zhang, Jaeyong Lee, Mo Tiwari, Azalia Mirhoseini</em></p>
 

<p>Efficient Hybrid Long Sequence Modeling with State Space Augmented Transformers <br/> <em>Denis X Charles, Eren Manavoglu, Jian Jiao, Jianfeng Gao, Simiao Zuo, Xiaodong Liu, Tuo Zhao</em></p>
 

<p>Does Collaborative Human–LM Dialogue Generation Help Information Extraction from Human–Human Dialogues? <br/> <em>Bo-Ru Lu, Chia-Hsuan Lee, Hao Cheng, Jean Utke, Mari Ostendorf, Nikita Haduong, Paul Koester, Tao Yu, Zeqiu Wu, Noah A. Smith</em></p>
 

<p>Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens <br/> <em>Hannaneh Hajishirzi, Jiacheng Liu, Luke Zettlemoyer, Sewon Min, Yejin Choi</em></p>
 

<p>RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation <br/> <em>Chunpu Xu, Hongyin Luo, Ruibin Yuan, Wei Xue, Yike Guo, Chi-Min Chan, Jie Fu</em></p>
 

<p>Exploring the Mystery of Influential Data for Mathematical Reasoning <br/> <em>Nan Duan, Weizhu Chen, Xinzhe Ni, Yeyun Gong, Yujiu Yang, Zhibin Gou, yelong shen</em></p>
 

<p>LalaEval: A Holistic Human Evaluation Framework for Domain-Specific Large Language Models <br/> <em>Chengfei Fu, Chongyan Sun, Hulong Wu, Ken Lin, Shiwei Wang, Zhen Wang</em></p>
 

<p>Trust No Bot? Personal Disclosures in Human-LLM Conversations <br/> <em>Golnoosh Farnadi, Maria Antoniak, Yash More, Yejin Choi, Niloofar Mireshghallah</em></p>
 

<p>Mamba: Linear-Time Sequence Modeling with Selective State Spaces <br/> <em>Albert Gu, Tri Dao</em></p>
 

<p>MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries <br/> <em>Yi Yang, Yixuan Tang</em></p>
 

<p>How bad is training on synthetic data? A statistical analysis of language model collapse <br/> <em>Merouane Abdelkader DEBBAH, Mohamed El Amine Seddik, Pierre Youssef, Soufiane Hayou, Suei-Wen Chen</em></p>
 

<p>V-STaR: Training Verifiers for Self-Taught Reasoners <br/> <em>Aaron Courville, Arian Hosseini, Rishabh Agarwal, Alessandro Sordoni, Nikolay Malkin, Xingdi Yuan</em></p>
 

<p>Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence <br/> <em>Alon Albalak, Atsushi Saito, Bingchen Zhao, Bo Peng, Daniel Goldstein, Eric Alcaide, Eugene Cheah, Fares Obeid, Guangyu Song, Haoqin Tu, Jian Zhu, Kranthi Kiran GV, Niklas Muennighoff, Qihang Zhao, Quentin Gregory Anthony, Ronald McClelland Jr., Rui-Jie Zhu, Ruichong Zhang, Satyapriya Krishna, Stella Biderman, Teddy Ferdinan, Haowen Hou</em></p>
 

<p>Linearizing Large Language Models <br/> <em>Achal Dave, Adrien Gaidon, Igor Vasiljevic, Jean Mercat, Kushal Arora, Sedrick Keh, Thomas Kollar</em></p>
 

<p>VideoDirectorGPT: Consistent Multi-Scene Video Generation via LLM-Guided Planning <br/> <em>Abhay Zala, Han Lin, Jaemin Cho, Mohit Bansal</em></p>
 

<p>OpenAgents: An Open Platform for Language Agents in the Wild <br/> <em>Caiming Xiong, Che Liu, Dongchan Shin, Fan Zhou, Hongjin SU, Junning Zhao, Luoxuan Weng, Peng Shi, Qian Liu, Tao Yu, Tianbao Xie, Toh Jing Hua, Yiheng Xu, Yitao Liu, Zeyu Leo Liu, Zhoujun Cheng</em></p>
 

<p>TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance <br/> <em>Chao Zhang, Haorui Wang, Lingkai Kong, Rongzhi Zhang, Xiusi Chen, Yinghao Li, Yuchen Zhuang</em></p>
 

<p>Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP <br/> <em>Alfiya Khabibullina, François Remy, Hayastan Avetisyan, Miryam de Lhoneux, Pieter Delobelle, Thomas Demeester</em></p>
 

<p>RAFT: Adapting Language Model to Domain Specific RAG <br/> <em>Ion Stoica, Joseph E. Gonzalez, Matei Zaharia, Naman Jain, Sheng Shen, Shishir G Patil, Tianjun Zhang</em></p>
 

<p>PhonATe: Impact of Type-Written Phonological Features of African American Language on Generative Language Modeling Tasks <br/> <em>Desmond U. Patton, Jessica A Grieser, Kathleen McKeown, Nicholas Deas, Shana Kleiner, Sreya Nandanampati, Tajh Martin, Xinmeng Hou</em></p>
 

<p>On the Geometry of Context and Word Embeddings in Next-Token-Prediction <br/> <em>Christos Thrampoulidis, Tina Behnia, Vala Vakilian, Yize Zhao</em></p>
 

<p>Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think <br/> <em>Barbara Plank, Bolei Ma, Chengzhi Hu, Paul Röttger, Xinpeng Wang</em></p>
 

<p>ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs <br/> <em>Adhithya Prakash Saravanan, Or Sharir, Pengrui Han, Rafal Dariusz Kocielnik, Roy Luoyao Jiang, Anima Anandkumar</em></p>
 

<p>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation <br/> <em>Biqing Qi, Bowen Zhou, Ermo Hua, Haoxiang Li, Hu Jinfang, Kai Tian, Kaiyan Zhang, Sihang Zeng, Zhang-Ren Chen</em></p>
 

<p>Resolving Knowledge Conflicts in Large Language Models <br/> <em>Heng Wang, Shangbin Feng, Tianxing He, Vidhisha Balachandran, Weijia Shi, Yike Wang, Yulia Tsvetkov</em></p>
 

<p>How Far Are We from Intelligent Visual Deductive Reasoning? <br/> <em>Jiatao Gu, Joshua M. Susskind, Navdeep Jaitly, Ruixiang ZHANG, Shuangfei Zhai, Yizhe Zhang, Richard He Bai</em></p>
 

<p>DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training <br/> <em>Anze Xie, Dacheng Li, Eric Xing, Hao Zhang, Ion Stoica, Joseph E. Gonzalez, Rulin Shao, Xuezhe Ma</em></p>
 

<p>Web Retrieval Agents for Evidence-Based Misinformation Detection <br/> <em>Hao Yu, Jacob-Junqi Tian, Jean-François Godbout, Kellin Pelrine, Mauricio Rivera, Mayank Goel, Reihaneh Rabbany, Tyler Vergho, Yury Orlovskiy, Zachary Yang</em></p>
 

<p>&#34;Task Success&#34; is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors <br/> <em>Denis Liu, Lin Guan, Subbarao Kambhampati, Yantian Zha, Yifan Zhou, Heni Ben Amor</em></p>
 

<p>Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images <br/> <em>Fan Xue, Jindong Gu, Philip Torr, Shuo Chen, Volker Tresp, Xun Xiao, Zefeng Wang, Zhen Han, Zifeng Ding</em></p>
 

<p>PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models <br/> <em>Maarten Sap, Priyanshu Kumar, Samuel Gehman, Thomas Hartvigsen, Xuhui Zhou, Devansh Jain</em></p>
 

<p>Reasoning about concepts with LLMs: Inconsistencies abound <br/> <em>Karthikeyan Natesan Ramamurthy, Maria Chang, Rosario Uceda Sosa, Moninder Singh</em></p>
 

<p>Logits of API-Protected LLMs Leak Proprietary Information <br/> <em>Matthew Finlayson, Swabha Swayamdipta, Xiang Ren</em></p>
 

<p>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking <br/> <em>Eric Zelikman, Georges Raif Harik, Nick Haber, Noah Goodman, Varuna Jayasiri, Yijia Shao</em></p>
 

<p>Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM <br/> <em>Baptiste Roziere, Hu Xu, Jacob Kahn, Jason E Weston, Olga Golovneva, Sainbayar Sukhbaatar, Shang-Wen Li, Vasu Sharma, Wen-tau Yih, Xi Victoria Lin, Xian Li</em></p>
 

<p>AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts <br/> <em>Jiahua Luo, Zefang Liu</em></p>
 

<p>Instruction-tuning Aligns LLMs to the Human Brain <br/> <em>Antoine Bosselut, Badr AlKhamissi, Khai Loong Aw, Martin Schrimpf, Syrielle Montariol</em></p>
 

<p>An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models <br/> <em>Emmy Liu, Graham Neubig, Jacob Andreas</em></p>
 

<p>Learning to Plan for Language Modeling from Unlabeled Data <br/> <em>Florian Mai, Marie-Francine Moens, Nathan Cornille</em></p>
 

<p>Impact of Preference Noise on the Alignment Performance of Generative Language Models <br/> <em>Dana Alon, Donald Metzler, Yang Gao</em></p>
 

<p>SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models <br/> <em>Dahua Lin, Haojie Duanmu, Jiangfei Duan, Xingcheng ZHANG, Xiuhong Li, Zhihang Yuan</em></p>
 

<p>Eliciting Latent Knowledge from &#34;Quirky&#34; Language Models <br/> <em>Alex Troy Mallen, Julia Kharchenko, Madeline Brumley, Nora Belrose</em></p>
 

<p>AmbigDocs: Reasoning across Documents on Different Entities under the Same Name <br/> <em>Eunsol Choi, Xi Ye, Yoonsang Lee</em></p>
 

<p>Is ChatGPT a Good Sentiment Analyzer? <br/> <em>Qiming Xie, Rui Xia, Yi Feng, Zengzhi Wang, Zinong Yang, Zixiang Ding</em></p>
 

<p>Multi-FAct for Multi-lingual Factuality Evaluation <br/> <em>Alice Oh, Eunsu Kim, Juhyun Oh, Sheikh Shafayat</em></p>
 

<p>Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models <br/> <em>Bang An, Furong Huang, Michael-Andrei Panaitescu-Liess, Sicheng Zhu, Yuancheng Xu, Ruiyi Zhang</em></p>
 

<p>LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset <br/> <em>Botao Yu, Frazier N. Baker, Huan Sun, Xia Ning, Ziqi Chen</em></p>
 

<p>Evaluating In-context Conversational Adaptation in Multimodal LLMs <br/> <em>Yilun Hua, Yoav Artzi</em></p>
 

<p>Rejection Improves Reliability: Training LLMs to Refuse Un-known Questions Using RL from Knowledge Feedback <br/> <em>Da Ma, Hongshen Xu, Kai Yu, Lu Chen, Shuai Fan, Situo Zhang, Zichen Zhu</em></p>
 

<p>Boundary detection in mixed AI-human texts <br/> <em>Dmitry Abulkhanov, Eduard Tulchinskii, German Magai, Irina Piontkovskaya, Kristian Kuznetsov, Sergey Nikolenko, Serguei Barannikov, Tatiana Gaintseva, Laida Kushnareva</em></p>
 

<p>CA-LoRA: Adapting Existing LoRA for Compressed LLMs to Enable Efficient Multi-Tasking on Personal Devices <br/> <em>Chen Chen, Kuai Li, Maosong Sun, TAO YANG, Weilin Zhao, Xu Han, Yuxiang Huang, Zhengyan Zhang, Zhiyuan Liu</em></p>
 

<p>Don&#39;t throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding <br/> <em>Andrew Cohen, Asli Celikyilmaz, Hannaneh Hajishirzi, Jiacheng Liu, Ramakanth Pasunuru, Yejin Choi</em></p>
 

<p>Crystal: Illuminating LLM Abilities on Language and Code <br/> <em>Bhargav M Kanakiya, Bowen Tan, Eric Xing, Hongyi Wang, Junbo Li, Natalia Vassilieva, Tianhua Tao, William Marshall, Zhengzhong Liu, Zhiqiang Shen, Joel Hestness</em></p>
 

<p>GeniL: A Multilingual Dataset on Generalizing Language <br/> <em>Aida Mostafazadeh Davani, Sagar Gubbi Venkatesh, Shachi Dave, Sunipa Dev, Vinodkumar Prabhakaran</em></p>
 

<p>RULER: What’s the Real Context Size of Your Long-Context Language Models? <br/> <em>Boris Ginsburg, Cheng-Ping Hsieh, Dima Rekesh, Fei Jia, Samuel Kriman, Shantanu Acharya, Simeng Sun</em></p>
 

<p>The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization <br/> <em>Arian Hosseini, Kashif Rasul, Lewis Tunstall, Michael Noukhovitch, Shengyi Huang, Weixun Wang</em></p>
 

<p>Can Language Models Solve Olympiad Programming? <br/> <em>Quan Shi, Karthik R Narasimhan, Michael Tang, Shunyu Yao</em></p>
 

<p>From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function <br/> <em>Chelsea Finn, Rafael Rafailov, Ryan Park, Joey Hejna</em></p>
 

<p>PRobELM: Plausibility Ranking Evaluation for Language Models <br/> <em>Andreas Vlachos, Chenxi Whitehouse, Eric Chamoun, Rami Aly, Moy Yuan</em></p>
 

<p>Bring Your Own Data! Self-Sensitivity Evaluation for Large Language Models <br/> <em>Aniruddha Saha, John Kirchenbauer, Jonas Geiping, Khalid Saifullah, Manli Shu, Micah Goldblum, Neel Jain, Tom Goldstein, Yuxin Wen</em></p>
 

<p>Can MLLMs Perform Text-to-Image In-Context Learning? <br/> <em>Hyung Il Koo, Kangwook Lee, Wonjun Kang, Yuchen Zeng, Yicong Chen</em></p>
 

<p>DeStein: Navigating Detoxification of Language Models via Universal Steering Pairs and Head-wise Activation Fusion <br/> <em>Chuanyang Gong, Han Jiang, Yu Li, Zhihua Wei</em></p>
 

<p>How Do In-Context Documents Affect Long-form Answer Generation? <br/> <em>Eunsol Choi, Fangyuan Xu, Hung-Ting Chen, Shane Arora</em></p>
 

<p>LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal Classification <br/> <em>Jiaming Shen, Junru Wu, Tianqi Liu, Xuanhui Wang, Zhen Qin</em></p>
 

<p>LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models <br/> <em>Furu Wei, Man Lan, Shaoguang Mao, Tao Ge, Ting Song, Wenshan Wu, Xun Wang, Yadong Zhang, Yan Xia</em></p>
 

<p>Do Large Language Models Have Compositional Ability? An Investigation into Scalability and Limitations <br/> <em>Yingyu Liang, Zhenmei Shi, Zhuoyan Xu</em></p>
 

<p>Decomposing Label Space, Format and Discrimination: Re- thinking How LLMs Respond and Solve Tasks via In-Context Learning <br/> <em>Quanyu Long, Wenya Wang, Yin Wu, Sinno Jialin Pan</em></p>
 

<p>Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports <br/> <em>Chenhao Tan, Danial Dervovic, Natraj Raman, Tianyu Cao</em></p>
 

<p>NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness <br/> <em>Abhijeet Awasthi, Aditya Kanade, Manav Singhal, Nagarajan Natarajan, Tushar Aggarwal</em></p>
 

<p>TarGEN: Targeted Data Generation with Large Language Models <br/> <em>Chitta Baral, Himanshu Gupta, Kevin Scaria, Mihir Parmar, Saurabh Arjun Sawant, Shreyas Verma, Swaroop Mishra, Ujjwala Anantheswaran</em></p>
 

<p>Uncovering Intermediate Variables in Transformers using Circuit Probing <br/> <em>Ellie Pavlick, Michael A. Lepori, Thomas Serre</em></p>
 

<p>UniMem: Towards a Unified View of Long-Context Large Language Models <br/> <em>Haolun Li, Junjie Fang, Likai Tang, Maosong Sun, Sen Song, Si Sun, Xiaodong Shi, Xin Cong, Yankai Lin, Yongjian Li, Yujia Qin, Yukun Yan, Zhenyu Li, Zhiyuan Liu, Hongzhe Bi</em></p>
 

<p>Towards Verifiable Text Generation with Symbolic References <br/> <em>Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Lucas Torroba Hennigen, Yoon Kim, Zejiang Shen</em></p>
 

<p>WebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? <br/> <em>Bill Yuchen Lin, Graham Neubig, Junpeng Liu, Wai Lam, Xiang Yue, Yifan Song, Yuanzhi Li</em></p>
 

<p>HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs <br/> <em>Anningzhe Gao, Benyou Wang, Chuyi Kong, Feng Jiang, Haizhou Li, Hongbo Zhang, Jianquan Li, Junying Chen, Ke Ji, Shunian Chen, Song Dingjie, Wenya Xie, Xiang Wan, Xidong Wang</em></p>
 

<p>LMD3: Language Model Data Density Dependence <br/> <em>Daphne Ippolito, David Andre, Garrett Honke, Gowthami Somepalli, John Kirchenbauer, Jonas Geiping, Katherine Lee, Tom Goldstein</em></p>
 

<p>The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models <br/> <em>Fred Morstatter, Jay Pujara, Jiarui Zhang, Kexuan Sun, Kian Ahrabian, Yifan Jiang, Zhivar Sourati</em></p>
 

<p>Tuning Language Models by Proxy <br/> <em>Alisa Liu, Xiaochuang Han, Yejin Choi, Yizhong Wang, Yulia Tsvetkov, Noah A. Smith</em></p>
 

<p>Evaluating LLMs at Detecting Errors in LLM Responses <br/> <em>Arman Cohan, Haoran Ranran Zhang, Nan Zhang, Rui Zhang, Ryo Kamoi, Salika Dave, Sarkar Snigdha Sarathi Das, Shaobo Qin, Sujeeth Reddy Vummanthala, Wenpeng Yin, Xiaoxin Lu, Yilun Zhao, Yusen Zhang, Jihyun Janice Ahn, Renze Lou</em></p>
 

<p>HDT: Hierarchical Document Transformer <br/> <em>Andreas Geiger, Haoyu He, Iryna Gurevych, Jan Buchmann, Markus Flicke</em></p>
 

<p>With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation <br/> <em>Deng Cai, Dongyang Ma, Yan Wang</em></p>
 

<p>CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text <br/> <em>Yang Yuan, Yiqun Yao, Zhenru Lin</em></p>
 

<p>Learning From Correctness Without Prompting Makes LLM Efficient Reasoner <br/> <em>Han Wu, Hanxu Hou, Linqi Song, Sichun Luo, Xiaojin Fu, Yuxuan Yao, Zhijiang Guo, Zhou Biyan, Jiahui Gao</em></p>
 

<p>Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph <br/> <em>Andrea Passerini, Bruno Lepri, Carlo Nicolini, Jacopo Staiano, Marco Bronzini</em></p>
 

<p>Scalable Model Editing via Customized Expert Networks <br/> <em>Ming Li, Tianyu Qi, Yu He, zihan yao</em></p>
 

<p>Fine-grained Hallucination Detection and Editing for Language Models <br/> <em>Abhika Mishra, Akari Asai, Graham Neubig, Hannaneh Hajishirzi, Vidhisha Balachandran, Yizhong Wang, Yulia Tsvetkov</em></p>
 

<p>Ontology-Guided Retrieval-Augmented Generation for Theme-Specific Entity Typing <br/> <em>Geeth De Mel, James Barry, Jiawei Han, Jinfeng Xiao, Linyi Ding, Mohab Elkaref</em></p>
 

<p>Unified View of Grokking, Double Descent and Emergent Abilities: A Comprehensive Study on Algorithm Task <br/> <em>Maosong Sun, Xu Han, Yufei Huang, Zhiyuan Liu, Shengding Hu</em></p>
 

<p>Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations <br/> <em>Aiping Xiong, Dongwon Lee, Eun-Ju Lee, Haeseung Seo, Mahjabin Nahar</em></p>
 

<p>Personalized Collaborative Fine-Tuning for On-Device Large Language Models <br/> <em>Dongyang Fan, Martin Jaggi, Nicolas Wagner</em></p>
 

<p>Benchmarks as Microscopes: A Call for Model Metrology <br/> <em>Ari Holtzman, Michael Saxon, Naomi Saphra, Peter West, William Yang Wang</em></p>
 

<p>Tabular Transfer Learning via Prompting LLMs <br/> <em>Jaehyun Nam, Jaehyung Kim, Jihoon Tack, Jinwoo Shin, Kyu Hwan Oh, Seong Hyeon Park, Sukmin Yun, Woomin Song</em></p>
 

<p>How Multilingual are Large Language Models Fine-tuned for Translation? <br/> <em>Aquia Richburg, Marine Carpuat</em></p>
 

<p>O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models <br/> <em>Deepeka Garg, Jared Vann, Mengda Xu, Sumitra Ganesh, Yanchao Sun, Yuchen Xiao, Udari Madhushani Sehwag</em></p>
 

<p>LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models <br/> <em>Adithya Samavedhi, Haodi Ma, Haotian Luo, Qiyue Gao, Shibo Hao, Shuhua Xie, Tianyang Liu, Xinyuan Wang, Xiyan Shao, Yi Gu, Zhen Wang, Zhiting Hu</em></p>
 

<p>Do Membership Inference Attacks Work on Large Language Models? <br/> <em>Anshuman Suri, David Evans, Hannaneh Hajishirzi, Luke Zettlemoyer, Michael Duan, Sewon Min, Weijia Shi, Yejin Choi, Yulia Tsvetkov, Niloofar Mireshghallah</em></p>
 

<p>Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics <br/> <em>Catherine Arnett, James A. Michaelov, Ben Bergen</em></p>
 

<p>The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets <br/> <em>Max Tegmark, Samuel Marks</em></p>
 

<p>Hummer: Towards Limited Competitive Preference Dataset <br/> <em>JUN ZHOU, Jingqing Ruan, Junwu Xiong, Li Jiang, Qingpei Guo, Xiaotie Deng, Yichuan Ding, Yusen Wu, zujie wen</em></p>
 

<p>Zephyr: Direct Distillation of LM Alignment <br/> <em>Alexander M Rush, Clémentine Fourrier, Edward Emanuel Beeching, Kashif Rasul, Leandro Von Werra, Lewis Tunstall, Nathan Habib, Nathan Lambert, Nathan Sarrazin, Omar Sanseviero, Shengyi Huang, Thomas Wolf, Younes Belkada, Nazneen Rajani</em></p>
 

<p>Nonparametric Variational Regularisation of Pretrained Transformers <br/> <em>Fabio James Fehr, James Henderson</em></p>
 

<p>Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability <br/> <em>Aaron T Parisi, Alex Rizkowsky, Alexander A Alemi, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Gaurav Mishra, Hanie Sedghi, Isabelle Simpson, Izzeddin Gur, Jaehoon Lee, Jasper Snoek, Jeffrey Pennington, Jiri Hron, John D Co-Reyes, Kathleen Kenealy, Kelvin Xu, Kevin Swersky, Laura A Culp, Lechao Xiao, Maxwell Bileschi, Noah Fiedel, Peter J Liu, Roman Novak, Rosanne Liu, Sharad Vikram, Simon Kornblith, Tris Warkentin, Azade Nova, Igor Mordatch, Jascha Sohl-Dickstein</em></p>
 

<p>Redesigning Information Markets in the Era of Language Models <br/> <em>Bernhard Schölkopf, Christopher Pal, Li Erran Li, Manuel Wuthrich, Martin Weiss, Nasim Rahaman, Yoshua Bengio</em></p>
 

<p>Large Language Model Routing with Benchmark Datasets <br/> <em>Anthony Ou, Justin Solomon, Kate Soule, Mikhail Yurochkin, Mírian Silva, Neil Thompson, Tal Shnitzer, Yuekai Sun</em></p>
 

<p>Language Models as Critical Thinking Tools: A Case Study of Philosophers <br/> <em>Amy X Zhang, Andre Ye, Jared Moore, Rose Novick</em></p>
 

<p>Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates <br/> <em>Avanika Narayan, Christopher Re, Kush Bhatia, Mayee F Chen</em></p>
 

<p>Prompt Exploration with Prompt Regression <br/> <em>Michael Feffer, Mikhail Yurochkin, Ronald Xu, Yuekai Sun</em></p>
 

<p>Evaluating faithfulness and content selection in book-length summarization <br/> <em>Aparna Garimella, Kyle Lo, Marzena Karpinska, Mohit Iyyer, Tanya Goyal, Varun Manjunatha, Yapei Chang, Yekyung Kim</em></p>
 

<p>Mapping the Increasing Use of LLMs in Scientific Papers <br/> <em>Christopher D Manning, Christopher Potts, Diyi Yang, Haley Lepp, Hancheng Cao, James Y. Zou, Sheng Liu, Siyu He, Weixin Liang, Wenlong Ji, Xuandong Zhao, Yaohui Zhang, Zhengxuan Wu, Zhi Huang</em></p>
 

<p>Scattered Mixture-of-Experts Implementation <br/> <em>Aaron Courville, Rameswar Panda, Shawn Tan, Yikang Shen</em></p>
 

<p>What Are Tools Anyway? A Survey from the Language Model Perspective <br/> <em>Daniel Fried, Graham Neubig, Hao Zhu, Zhiruo Wang, Zhoujun Cheng</em></p>
 

<p>A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration <br/> <em>Diyi Yang, Peng Li, Yang Liu, Yanzhe Zhang, Zijun Liu</em></p>
 

<p>Data Checklist: On Unit-Testing Datasets with Usable Information <br/> <em>Dan Jurafsky, Kawin Ethayarajh, Shabnam Behzad, Heidi Chenyu Zhang</em></p>
 

<p>MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs <br/> <em>Arianna Bisazza, Raquel Fernández, Vera Neplenbroek</em></p>
 

<p>MambaByte: Token-free Selective State Space Model <br/> <em>Alexander M Rush, Jing Nathan Yan, Junxiong Wang, Tushaar Gangavarapu</em></p>
 

<p>Description-Based Text Similarity <br/> <em>Amir David Nissan Cohen, Avshalom Manevich, Shauli Ravfogel, Valentina Pyatkin, Yoav Goldberg</em></p>
 

<p>Generating Synthetic Datasets for Few-shot Prompt Tuning <br/> <em>Boyang Li, Chunyan Miao, Xu Guo, Zilin Du</em></p>
 

<p>Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in Subjective Tasks? <br/> <em>Antske Fokkens, Eric Nalisnick, Swabha Swayamdipta, Urja Khurana</em></p>
 

<p>Does RoBERTa Perform Better than BERT in Continual Learning: An Attention Sink Perspective <br/> <em>Niranjan Balasubramanian, Xueying Bai, Yifan Sun</em></p>
 

<p>An In-Context Learning Agent for Formal Theorem-Proving <br/> <em>Amitayush Thakur, George Tsoukalas, Jimmy Xin, Swarat Chaudhuri, Yeming Wen</em></p>
 

<p>Efficient Parallelization Layouts for Large-Scale Distributed Model Training <br/> <em>Johannes Hagemann, Konstantin Dobler, Maximilian Schall, Samuel Weinbach, Gerard de Melo</em></p>
 

<p>Unforgettable Generalization in Language Models <br/> <em>Eric Zhang, Jacob Andreas, Leshem Choshen</em></p>
 

<p>MileBench: Benchmarking MLLMs in Long Context <br/> <em>Benyou Wang, Fei Yu, Guiming Hardy Chen, Shunian Chen, Song Dingjie, Xiang Wan</em></p>
 

<p>AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs <br/> <em>Huan Sun, Zeyi Liao</em></p>
 

<p>List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs <br/> <em>An Yan, Jianfeng Gao, Jianfeng Wang, Julian McAuley, Junda Wu, Kevin Lin, Lijuan Wang, Linjie Li, Wanrong Zhu, Zhengyuan Yang, Jianwei Yang</em></p>
 

<p>Source-Aware Training Enables Knowledge Attribution in Language Models <br/> <em>David Wadden, Emma Strubell, Iz Beltagy, Lu Wang, Muhammad Khalifa, Hao Peng, Honglak Lee</em></p>
 

<p>A Language Agent for Autonomous Driving <br/> <em>Jiageng Mao, Junjie Ye, Marco Pavone, Yue Wang, Yuxi Qian</em></p>
 

<p>Auxiliary task demands mask the capabilities of smaller language models <br/> <em>Jennifer Hu, Michael Frank</em></p>
 

<p>LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition <br/> <em>Bill Yuchen Lin, Chao Du, Chengsong Huang, Min Lin, Qian Liu, Tianyu Pang</em></p>
 

<p>GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark <br/> <em>Asa Cooper Stickland, Betty Li Hou, David Rein, Jackson Petty, Julian Michael, Julien Dirani, Richard Yuanzhe Pang, Samuel R. Bowman</em></p>
 

<p>Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms <br/> <em>Michael Hanna, Sandro Pezzelle, Yonatan Belinkov</em></p>
 

<p>Stronger Random Baselines for In-Context Learning <br/> <em>David Mimno, Gregory Yauney</em></p>
 

<p>Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities <br/> <em>Hirai Shota, Hiroki Iida, Kakeru Hattori, Kazuki Fujii, Masanari Ohi, Mengsay Loem, Naoaki Okazaki, Rio Yokota, Sakae Mizuki, Taishi Nakamura</em></p>
 

<p>Decoupling Noise and Toxic Parameters for Language Model Detoxification by Task Vector Merging <br/> <em>Takeshi Kojima, Yongmin Kim, Yusuke Iwasawa, Yutaka Matsuo</em></p>
 

<p>Optimising Calls to Large Language Models with Uncertainty-Based Two-Tier Selection <br/> <em>Alexandra Birch, Guillem Ramírez, Ivan Titov</em></p>
 

<p>Adaptive Quantization Error Reconstruction for LLMs with Mixed Precision <br/> <em>Chuzhan Hao, Hao Henry Wang, Jinpeng Xia, Lin Ou, Yuewei Zhang</em></p>
 

<p>StyleTalker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation <br/> <em>Ge Zhu, Jordan Darefsky, Nima Mesgarani, Xilin Jiang, Yinghao Aaron Li</em></p>
 

<p>Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images <br/> <em>Ali Naseh, Amir Houmansadr, Katherine Thai, Mohit Iyyer</em></p>
 

<p>Compression Represents Intelligence Linearly <br/> <em>Jinghan Zhang, Junxian He, Yuzhen Huang, Zifei Shan</em></p>
 

<p>Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping <br/> <em>Lucas Lehnert, Michael Rabbat, Paul McVay, Qinqing Zheng, Sainbayar Sukhbaatar, Yuandong Tian, DiJia Su</em></p>
 

<p>How Easily do Irrelevant Inputs Skew the Responses of Large Language Models? <br/> <em>Jian Xie, Jiangjie Chen, Kai Zhang, Siye Wu, Tinghui Zhu, Yanghua Xiao</em></p>
 

<p>Embedding Nationality Context into LLM-Based Human Response Simulations <br/> <em>Lewis Griffin, Louis Kwok, Michal Bravansky</em></p>
 

<p>Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning <br/> <em>Jian Xie, Kai Zhang, Tinghui Zhu, Yu Su</em></p>
 

<p>LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory <br/> <em>Andrew Lo, Jillian Ross, Yoon Kim</em></p>
 

<p>CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias <br/> <em>Hugo Laurençon, Pranav Narayanan Venkit, Rebecca J. Passonneau, Shomir Wilson, Vipul Gupta</em></p>
 

<p>Chinese Tiny LLM: Pretraining a Chinese-Centered Large Language Model <br/> <em>Cheng Yuyang, Ding Pan, Ge Zhang, Guorui Zhou, Jiaheng Liu, Ruibin Yuan, Songyang Gao, Tianyu Zheng, Xinchen Luo, Xingwei Qu, Zhouliang Yu, Ziyang Ma, Wenhu Chen, Xeron Du</em></p>
 

<p>Using Natural Language Explanations to Rescale Human Judgments <br/> <em>Greg Durrett, Jifan Chen, Junyi Jessy Li, Manya Wadhwa</em></p>
 

<p>LLM360: Towards Fully Transparent Open-Source LLMs <br/> <em>Aurick Qiao, Bowen Tan, Cun Mu, Eric Xing, Fajri Koto, Guowei He, Haonan Li, Hongyi Wang, Junbo Li, Liping Tang, Mark Schulze, Nikhil Ranjan, Omkar Pangarkar, Preslav Nakov, Richard Fan, Roberto Iriondo, Suqi Sun, Tianhua Tao, Timothy Baldwin, Victor Miller, Willie Neiswanger, Yi Gu, Yonghao Zhuang, Yuqi Wang, Zhengzhong Liu, Zhiqiang Shen, Zhiting Hu</em></p>
 

<p>From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsies <br/> <em>Adam Visokay, Jeffrey T. Leek, Kentaro Hoffman, Li Liu, Shuxian Fan, Stephen Salerno, Tyler McCormick</em></p>
 

<p>The Larger the Better? Improved LLM Code-Generation via Budget Reallocation <br/> <em>Jonas Gehring, Michael Hassid, Roy Schwartz, Tal Remez, Yossi Adi</em></p>
 

<p>&#34;Merge Conflicts!&#39;&#34; Exploring the Impacts of External Knowledge Distractors to Parametric Knowledge Graphs <br/> <em>Cheng Qian, Tongshuang Wu, Xinran Zhao</em></p>
 

<p>Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models <br/> <em>Adam Karvonen</em></p>
 

<p>AgentKit: Structured LLM Reasoning with Dynamic Graphs <br/> <em>Shrimai Prabhumoye, So Yeon Min, Stephen Marcus McAleer, Tom Mitchell, Yewen Fan, Yonatan Bisk, Yuanzhi Li, Yue Wu, Russ Salakhutdinov</em></p>
 

<p>A Reparameterized Discrete Diffusion Model for Text Generation <br/> <em>Jianbo Yuan, Lei Yu, Lin Zheng, Lingpeng Kong</em></p>
 

<p>Best Practices and Lessons Learned on Synthetic Data <br/> <em>Chenglei Si, Daiyi Peng, Diyi Yang, Fangyu Liu, Jinmeng Rao, Ruibo Liu, Yanzhe Zhang, Andrew M. Dai, Denny Zhou, Jerry Wei, Steven Zheng</em></p>
 

<p>Let’s Think Dot by Dot: Hidden computation in transformer language models <br/> <em>Jacob Pfau, Samuel R. Bowman, William Merrill</em></p>
 

<p>Multi-hop Question Answering under Temporal Knowledge Editing <br/> <em>Di Wang, Gang Lin, Haoyang Fei, Keyuan Cheng, Lijie Hu, Lu Yu, Muhammad Asif Ali, Yuxuan Zhai</em></p>
 

<p>DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning <br/> <em>Abhay Zala, Han Lin, Jaemin Cho, Mohit Bansal</em></p>
 

<p>Autonomous Evaluation and Refinement of Digital Agents <br/> <em>Alane Suhr, Jiayi Pan, Nicholas Tomlin, Sergey Levine, Yichi Zhang, Yifei Zhou</em></p>
 

<p>Building a Large Japanese Web Corpus for Large Language Models <br/> <em>Hirai Shota, Hiroki Iida, Kakeru Hattori, Kazuki Fujii, Masanari Ohi, Mengsay Loem, Naoaki Okazaki, Rio Yokota, Sakae Mizuki, Taishi Nakamura</em></p>
 

<p>Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck <br/> <em>Benoît Sagot, Nathan Godey, Éric Villemonte de la Clergerie</em></p>
 

<p>Are Language Models Robust Coreference Resolvers? <br/> <em>Alan Ritter, Nghia T. Le</em></p>
 

<p>Information Guided Regularization for Fine-tuning Language Models <br/> <em>Mandar Sharma, Naren Ramakrishnan, Nikhil Muralidhar, Raquib Bin Yousuf, Shengzhe Xu</em></p>
 

<p>Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning <br/> <em>Licong Lin, Ruiqi Zhang, Song Mei, Yu Bai</em></p>
 

<p>Generating Probabilistic Scenario Programs from Natural Language <br/> <em>Alberto Sangiovanni-Vincentelli, Ana Cismaru, Devan Shanker, Karim Elmaaroufi, Marcell Vazquez-Chanlatte, Matei Zaharia, Sanjit A. Seshia</em></p>
 

<p>Your Context Is Not an Array: Unveiling Random Access Limitations in Transformers <br/> <em>MohammadReza Ebrahimi, Roland Memisevic, Sunny Panchal</em></p>
 

<p>Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense? <br/> <em>Dan Roth, Muyu He, Xingyu Fu, Yujie Lu, William Yang Wang</em></p>
 

<p>From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples <br/> <em>Mihai Surdeanu, Robert Vacareanu, Vasile Suciu, Vlad Andrei Negru</em></p>
 

<p>Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey <br/> <em>Barbara Plank, Philipp Mondorf</em></p>
 

<p>Forklift: An Extensible Neural Lifter <br/> <em>Jackson Woodruff, Jordi Armengol-Estapé, Michael O&#39;Boyle, Rodrigo C. O. Rocha, Pasquale Minervini</em></p>
 

<p>Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training <br/> <em>Danqi Chen, Mengzhou Xia, Mike Lewis, Zexuan Zhong</em></p>
 

<p>What makes a good metric? Meta-evaluating automatic metrics for text-to-image consistency <br/> <em>Adina Williams, Candace Ross, Melissa Hall, Adriana Romero-Soriano</em></p>
 

<p>Empowering Large Language Model Agents through Action Learning <br/> <em>Guoyin Wang, Haiteng Zhao, Hongxia Yang, Jing Su, Jingjing Xu, Lingpeng Kong, Zhi-Hong Deng, Chang Ma</em></p>
 

<p>On Limitations of the Transformer Architecture <br/> <em>Binghui Peng, Christos Papadimitriou, Srini Narayanan</em></p>
 

<p>IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations <br/> <em>Bhuwan Dhingra, Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Robin Jia, Ruohao Guo, Willie Neiswanger, Dani Yogatama</em></p>
 

<p>On Fairness of Low-Rank Adaptation of Large Models <br/> <em>Berivan Isik, Pura Peetathawatchai, Zhoujie Ding, Ken Liu, Sanmi Koyejo</em></p>
 

<p>Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning <br/> <em>Amer Sinha, Badih Ghazi, Chiyuan Zhang, Daogao Liu, Lynn Chua, Pasin Manurangsi, Pritish Kamath, Ravi Kumar, Yangsibo Huang</em></p>
 

<p>Information-Theoretic Distillation for Reference-less Summarization <br/> <em>Faeze Brahman, Jaehun Jung, Liwei Jiang, Pang Wei Koh, Peter West, Ximing Lu, Yejin Choi</em></p>
 

<p>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders <br/> <em>Dzmitry Bahdanau, Marius Mosbach, Nicolas Chapados, Parishad BehnamGhader, Siva Reddy, Vaibhav Adlakha</em></p>
 

<p>Faithful and Unfaithful Error Recovery in Chain of Thought <br/> <em>Alice Li, Chenyu Tang, Evelyn Yee, Leon Bergen, Ramamohan Paturi, Yeon Ho Jung</em></p>
 

<p>Interpretable Gradient-Based Adversarial Attacks on Large Language Models <br/> <em>Ani Nenkova, Bang An, Furong Huang, Gang Wu, Joe Barrow, Sicheng Zhu, Tong Sun, Zichao Wang, Ruiyi Zhang</em></p>
 

<p>Beyond Correctness: Exercising Language Models for Efficient Code Generation <br/> <em>Jiawei Liu, Junhao Wang, LINGMING ZHANG, Songrun Xie, Yifeng Ding, Yuxiang Wei</em></p>
 

<p>Early Weight Averaging meets High Learning Rates for LLM Pre-training <br/> <em>Abhishek Kumar, Atula Tejaswi, Jean Kaddour, Sunny Sanyal, Sujay Sanghavi</em></p>
 

<p>Chain-of-Symbol Prompting For Spatial Reasoning in Large Language Models <br/> <em>Hanxu Hu, Hongyuan Lu, Huajian Zhang, Wai Lam, Yue Zhang, Yun-Ze Song</em></p>
 

<p>What&#39;s in Your &#34;Safe&#34; Data?: Identifying Benign Data that Breaks Safety <br/> <em>Luxi He, Mengzhou Xia, Peter Henderson</em></p>
 

<p>TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding <br/> <em>Beidi Chen, Hanshi Sun, Xinyu Yang, Yuandong Tian, Zhuoming Chen</em></p>
 

<p>Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models <br/> <em>Besmira Nushi, Harsha Nori, Rich Caruana, Sebastian Bordt, Vanessa Cristiny Rodrigues Vasconcelos</em></p>
 

<p>Reverse Training to Nurse the Reversal Curse <br/> <em>Jason E Weston, Olga Golovneva, Sainbayar Sukhbaatar, Zeyuan Allen-Zhu</em></p>
 

<p>LLM4Causal: Large Language Model for Causal Decision Making <br/> <em>Haitao Jiang, Jianian Wang, Lin Ge, Rui Song, Yuhe Gao</em></p>
 

<p>Starling-7B: Improving Helpfulness and Harmlessness with RLAIF <br/> <em>Banghua Zhu, Evan Frick, Hanlin Zhu, Jian Zhang, Jiantao Jiao, Karthik Ganesan, Tianhao Wu, Wei-Lin Chiang</em></p>
 

<p>RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models <br/> <em>Bryan Catanzaro, Jie Huang, Kevin Chang, Mohammad Shoeybi, Peng Xu, Wei Ping</em></p>
 

<p>JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks <br/> <em>Siyuan Ma, Weidi Luo, Xiaogeng Liu, Xiaoyu Guo, Chaowei Xiao</em></p>
 

<p>A Long Way to Go: Investigating Length Correlations in RLHF <br/> <em>Greg Durrett, Jiacheng Xu, Prasann Singhal, Tanya Goyal</em></p>
 

<p>Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers <br/> <em>Andy Yang, David Chiang</em></p>
 

<p>CoLLEGe: Concept Embedding Generation for Large Language Models <br/> <em>Brenden M. Lake, Mengye Ren, Ryan Teehan</em></p>
 

<p>CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration <br/> <em>Han Wu, Lanqing HONG, Lingpeng Kong, Renjie Pi, Tianyang Han, Xin Jiang, Zhenguo Li, Jiahui Gao</em></p>
 

<p>Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding <br/> <em>Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, Rishab Parthasarathy, William Brandon, Zachary Ankner</em></p>
 

<p>MONAL: Model Autophagy Analysis for Modeling Human-AI Interactions <br/> <em>Di Wang, Lijie Hu, Lu Yu, Muhammad Asif Ali, Shu Yang</em></p>
 

<p>EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents <br/> <em>Abhay Zala, Han Lin, Jaehong Yoon, Jaemin Cho, Mohit Bansal</em></p>
 

<p>Massive Activations in Large Language Models <br/> <em>Mingjie Sun, Xinlei Chen, J Zico Kolter, Zhuang Liu</em></p>
 

<p>Suspicion Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4 <br/> <em>Bill Yuchen Lin, Bo Yang, Jiaxian Guo, Paul Yoo, Yusuke Iwasawa, Yutaka Matsuo</em></p>
 

<p>Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models <br/> <em>Jeff Z. Pan, Jie He, Simon Chi Lok Yu, Pasquale Minervini</em></p>
 

<p>StructLM: Towards Building Generalist Models for Structured Knowledge Grounding <br/> <em>Alex Zhuang, Ge Zhang, Junjie Wang, Tianyu Zheng, Weiming Ren, Wenhao Huang, Xiang Yue, Xeron Du, Jie Fu, Wenhu Chen</em></p>
 

<p>D2PO: Discriminator-Guided DPO with Response Evaluation Models <br/> <em>Greg Durrett, Nathan Lambert, Prasann Singhal, Scott Niekum, Tanya Goyal</em></p>
 

<p>Tower: An Open Multilingual Large Language Model for Translation-Related Tasks <br/> <em>Amin Farajian, Andre Martins, Ben Peters, Duarte Miguel Alves, José Pombal, João Alves, Nuno M Guerreiro, Patrick Fernandes, Pedro Henrique Martins, Pierre Colombo, Sweta Agrawal, José G. C. de Souza, Ricardo Rei</em></p>
 

<p>Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models <br/> <em>Bowen Zhang, Haotian Zhang, Haoxuan You, Hong-You Chen, Philipp Dufter, Shih-Fu Chang, Tsu-Jui Fu, Yinfei Yang, Zhe Gan, Chen Chen, William Yang Wang</em></p>
 

<p>Self-Guide: Better Task-Specific Instruction Following via Self-Synthetic Finetuning <br/> <em>Chenyang Zhao, Graham Neubig, Tongshuang Wu, Vijay Viswanathan, Xueying Jia</em></p>
 

<p>3M-Diffusion: Latent Multi-Modal Diffusion for Language-Guided Molecule Generation <br/> <em>Huaisheng Zhu, Vasant G Honavar, Teng Xiao</em></p>
 

<p>CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting <br/> <em>Huihan Li, Liwei Jiang, Nouha Dziri, Xiang Ren, Yejin Choi</em></p>
 

<p>LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models <br/> <em>Haoran Li, Huaxiu Yao, Junqi Liu, Shiyuan Luo, Xiaowei Jia, Zexian Wang</em></p>
 

<p>CTIKG: LLM-Powered Knowledge Graph Construction from Cyber Threat Intelligence <br/> <em>Liangyi Huang, Xusheng Xiao</em></p>
 

<p>Enhancing Adversarial Robustness of LLMs with Analytic Hierarchy Process <br/> <em>Jiahao Zhao, Minzheng Wang, Nan Xu, Wenji Mao, YinLuo</em></p>
 

<p>Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions <br/> <em>Abby Brennan-Jones, Akul Sethi, Anton Lozhkov, Arjun Guha, Carolyn Jane Anderson, Edward Berman, Federico Cassano, George Chakhnashvili, Jacob Ginesin, Luisa Li, Noah Shinn</em></p>
 

<p>Length-Controlled AlpacaEval: A Simple Debiasing of Automatic Evaluators <br/> <em>Percy Liang, Tatsunori Hashimoto, Yann Dubois</em></p>
 

<p>STaR-GATE: Teaching Language Models to Ask Clarifying Questions <br/> <em>Chinmaya Andukuri, Jan-Philipp Fränken, Noah Goodman, Tobias Gerstenberg</em></p>
 

<p>Should We Attend More or Less? Modulating Attention for Fairness <br/> <em>Abdelrahman Zayed, Goncalo Mordido, Samira Shabanian, Sarath Chandar</em></p>
 

<p>On Robustness-Accuracy Characterization of Language Models using Synthetic Datasets <br/> <em>Ching-Yun Ko, Luca Daniel, Payel Das, Pin-Yu Chen, Yung-Sung Chuang</em></p>
 

<p>Handling Open-Vocabulary Constructs in Formalizing Specifications: Retrieval Augmented Parsing with Expert Knowledge <br/> <em>Dhruv Verma, Erez Zadok, Geoff Kuenning, Mohammad Saqib Hasan, Niranjan Balasubramanian, Sayontan Ghosh, Scott Smolka</em></p>
 

<p>Do Language Models Plan Ahead for Future Tokens? <br/> <em>John Xavier Morris, Lionel Levine, Wilson Wu</em></p>
 

<p>Automata-based constraints for language model decoding <br/> <em>Frederick Liu, Luheng He, Terry Koo</em></p>
 

<p>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversations <br/> <em>Ahmed Hassan Awadallah, Beibin Li, Chi Wang, Doug Burger, Erkang Zhu, Gagan Bansal, Jiale Liu, Jieyu Zhang, Li Jiang, Ryen W White, Shaokun Zhang, Xiaoyun Zhang, Yiran Wu, Qingyun Wu</em></p>
 

<p>TOFU: A Task of Fictitious Unlearning for LLMs <br/> <em>Avi Schwarzschild, Pratyush Maini, Zachary Chase Lipton, Zhili Feng, J Zico Kolter</em></p>
 

<p>SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design <br/> <em>Aakanksha Naik, Carl Edwards, Heng Ji, Martin D. Burke, Tom Hope, Tushar Khot</em></p>
 

<p>Inspecting and Editing Knowledge Representations in Language Models <br/> <em>Evan Hernandez, Jacob Andreas, Belinda Z. Li</em></p>
 

<p>Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators <br/> <em>Anna Korhonen, Ehsan Shareghi, Han Zhou, Ivan Vulić, Nigel Collier, Yinhong Liu, Zhijiang Guo</em></p>
 

<p>CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs <br/> <em>Huan Ma, Jialuo Li, Jingzhe Shi, Lei Li, Qinwei Ma, Zaiwen Yang</em></p>
 

<p>Forcing Diffuse Distributions out of Language Models <br/> <em>Avi Schwarzschild, Daphne Ippolito, Nicholas Carlini, Yiming Zhang, J Zico Kolter</em></p>
 

<p>Certifying LLM Safety against Adversarial Prompting <br/> <em>Aaron Jiaxun Li, Aounon Kumar, Chirag Agarwal, Soheil Feizi, Suraj Srinivas, Himabindu Lakkaraju</em></p>
 

<p>Latent Causal Probing: A Formal Perspective on Probing with Causal Models of Data <br/> <em>Charles Jin</em></p>
 

<p>TMMLU+: An Improved Traditional Chinese Evaluation Suite for Foundation Models <br/> <em>Hong-Han Shuai, Jun-Da Chen, Sega Cheng, Wei Min Chu, Ya Ting Pai, Zhi Rui Tam, Yen-Wei Lee</em></p>
 

<p>BumbleBee: Dynamic KV Cache Summarization in Transformers using Submodular Optimization <br/> <em>Anthony Rowe, Jeff Bilmes, Lilly Kumari, Nikhil Sarda, Shengjie Wang, Tianyi Zhou</em></p>
 

<p>Keep the Cost Down: A Review on Methods to Optimize LLM’s KV-Cache Consumption <br/> <em>Hongyi Zhang, Shi Luohe, Yao Yao, Zuchao Li, hai zhao</em></p>
 

<p>PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models <br/> <em>Jesse Thaler, Siddharth Mishra-Sharma, YIDING SONG</em></p>
 

<p>IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models <br/> <em>Abhik Bhattacharjee, Haz Sameen Shahgir, Khondker Salman Sayeed, Rifat Shahriyar, Wasi Uddin Ahmad, Yue Dong</em></p>
 

<p>Yes, no, maybe? Revisiting language models&#39; response stability under paraphrasing for the assessment of political leaning <br/> <em>Jannis Vamvas, Lena Ann Jäger, Patrick Haller</em></p>
 

<p>Measuring Taiwanese Mandarin Language Understanding <br/> <em>Po-Heng Chen, Sijia Cheng, Wei-Lin Chen, Yen-Ting Lin, Yun-Nung Chen</em></p>
 

<p>Pairwise Proximal Policy Optimization: Language Model Alignment with Comparative RL <br/> <em>Banghua Zhu, Jiantao Jiao, Kannan Ramchandran, Ruoyu Zhang, Tianhao Wu, Zhaojin Wen</em></p>
 

<p>Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness <br/> <em>Hongming Zhang, Sihao Chen, Tong Chen, Tongshuang Wu, Xinran Zhao</em></p>
 

<p>MouSi: Poly-Visual-Expert Vision-Language Models <br/> <em>Boyang Hong, Guodong Zheng, Hang Yan, Caishuang Huang, Junjie Ye, Junke Wang, Lu Chen, Ming Zhang, Qi Zhang, Rui Zheng, Senjie Jin, Sirui Song, Tao Gui, Tao Ji, Xiaoran Fan, Xipeng Qiu, Xuanjing Huang, Yu-Gang Jiang, Yuhao Zhou, Zhiheng Xi, Zuxuan Wu, 江常皓, Shihan Dou, Shuo Li</em></p>
 

<p>Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration <br/> <em>Lingpeng Kong, Qiushi Sun, Xiang Li, Xipeng Qiu, Zhangyue Yin, Zhiyong Wu</em></p>
 

<p>MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models <br/> <em>Hongyuan Mei, Jiading Fang, Jing Li, Kangrui Wang, Matthew Walter, Mo Yu, Peng Ding, Peng Li, Xiaochen Zhou</em></p>
 

<p>ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning <br/> <em>Alan Yuille, Yuxuan Wang, Zhuowan Li, Zilong Zheng</em></p>
 

<p>Measuring and Controlling Instruction (In)Stability in Language Model Dialogs <br/> <em>David Bau, Fernanda Viégas, Hanspeter Pfister, Kenneth Li, Martin Wattenberg, Naomi Bashkansky, Tianle Liu</em></p>
 

<p>Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking <br/> <em>Ahmad Beirami, Alekh Agarwal, Alexander D&#39;Amour, Chirag Nagpal, Deepak Ramachandran, Jacob Eisenstein, Jonathan Berant, Katherine A Heller, Peter Shaw, Stephen R Pfohl, Adam Fisch, Krishnamurthy Dj Dvijotham</em></p>
 

<p>SteP: Stacked LLM Policies for Web Actions <br/> <em>Paloma Sodhi, Ryan McDonald, S.R.K Branavan, Yoav Artzi</em></p>
 

<p>LLM-Datasets: An Open Framework for Pretraining Datasets of Large Language Models <br/> <em>Georg Rehm, Lucas Fonseca Lage, Malte Ostendorff, Pedro Ortiz Suarez</em></p>
 

<p>Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization <br/> <em>Costas Mavromatis, George Karypis, Petros Karypis</em></p>
 

<p>Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners <br/> <em>Boseop Kim, Dain Kim, Doohae Jung, Jihyeon Lee, Kyoung-Woon On</em></p>
 

<p>Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data <br/> <em>Andrey Gromov, Apratim Dey, Dan Roberts, David L. Donoho, Dhruv Bhandarkar Pai, Diyi Yang, Henry Sleight, John Hughes, Matthias Gerstgrasser, Rafael Rafailov, Rajashree Agrawal, Rylan Schaeffer, Tomasz Korbak, Sanmi Koyejo</em></p>
 

<p>Prompt-prompted Mixture of Experts for Efficient LLM Generation <br/> <em>Beidi Chen, Harry Dong, Yuejie Chi</em></p>
 

<p>WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting <br/> <em>Bertie Vidgen, Olly Styles, Patricio Cerda-Mardini, Sam Miller, Tanaya Guha, Victor Sanchez</em></p>
 

<p>Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation <br/> <em>Adam Tauman Kalai, Eliana Lorch, Eric Zelikman, Lester Mackey</em></p>
 

<p>Cohesive Conversations: Enhancing Authenticity in Multi-Agent Simulated Dialogues <br/> <em>Hideki Nakayama, KuanChao Chu, Yi-Pei Chen</em></p>
 

<p>StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows <br/> <em>Chi Wang, Shaokun Zhang, Tianwei Yue, Yiran Wu, Qingyun Wu</em></p>
 

<p>MiniCPM: Unveiling the Potential of Small Language Models <br/> <em>Chao Jia, Chaoqun He, Chenyang Zhao, Chongyi Wang, Ganqu Cui, Guoyang Zeng, Jie Cai, Jie Zhou, Maosong Sun, Ning Ding, Weilin Zhao, Xiang Long, Xinrong Zhang, Xu Han, Yewei Fang, Yuan Yao, Yuge Tu, Yuxiang Huang, Zhen Leng Thai, Zhi Zheng, Zhiyuan Liu, Zhongwu Zhai, dahai li, Shengding Hu</em></p>
 

<p>Timo: Towards Better Temporal Reasoning for Language Models <br/> <em>Jun Zhang, Juntao Li, Min zhang, Tong Zhu, Xiaoye Qu, Yu Cheng, Zhaochen Su</em></p>
 

<p>Bot or Human? Detecting ChatGPT Imposters with A Single Question <br/> <em>Hong Wang, Melody Yu, Weizhi Wang, Xifeng Yan, Xuan Luo</em></p>
 

<p>Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs <br/> <em>Frederick L. Oswald, Hanjie Chen, Pengda Wang, Zilin Xiao</em></p>
 

<p>Enhancing Language Models with Idiomatic Reasoning <br/> <em>Hongyu Gong, Jianing Zhou, Suma Bhat, Ziheng Zeng</em></p>
 

<p>ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation <br/> <em>Ana Brassard, Benjamin Heinzerling, Keisuke Sakaguchi, Keito Kudo, Kentaro Inui</em></p>
 

<p>ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction <br/> <em>Boming Kang, Kaixiong Zhou, Mengnan Du, Mingyu Jin, Ruosong Ye, Yongfeng Zhang, Zhenting Wang, Haochen Xue</em></p>
 

<p>Stream of Search (SoS): Learning to Search in Language <br/> <em>Archit Sharma, Denise H J Lee, Gabriel Grand, Muxin Liu, Noah Goodman, Winson Cheng, Kanishk Gandhi</em></p>
 

<p>Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation <br/> <em>Declan Grabb, Max Lamparth, Nina Vasan</em></p>
 

<p>Prompt Public Large Language Models to Synthesize Data for Private On-device Applications <br/> <em>Daniel Ramage, Shanshan Wu, Yanxiang Zhang, Yuanbo Zhang, Zheng Xu</em></p>
 

<p>Agent-DocEdit: Language-Instructed LLM Agent for Content-Rich Document Editing <br/> <em>Puneet Mathur, Rajiv Jain, Te-Lin Wu, Vlad I Morariu, Yufan Zhou</em></p>
 

<p>From Strategic Narratives to Code-Like Cognitive Models: An LLM-Based Approach in A Sorting Task <br/> <em>Hanbo Xie, Robert Wilson, Hua-Dong Xiong</em></p>
 

<p>See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses <br/> <em>Chenguang Zhu, Jianhao Yan, Ming Zhong, Xuefeng Bai, Yang Liu, Yinghao Yang, Yue Zhang, Yulong Chen, Ziyi Yang</em></p>
 

<p>SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval <br/> <em>Griffin Thomas Adams, Jason Zucker, Noémie Elhadad</em></p>
 

<p>Effective Prompt Extraction from Language Models <br/> <em>Daphne Ippolito, Nicholas Carlini, Yiming Zhang</em></p>
 

<p>ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for Contrastive Self-Training <br/> <em>Ji Zhang, Ming Yan, Peng Li, Yang Liu, Zonghan Yang, Fei Huang</em></p>
 

<p>InstructAV: Instruction Fine-tuning Large Language Models for Authorship Verification <br/> <em>Chun Wei Seah, Roy Ka-Wei Lee, Yujia Hu, Zhiqiang Hu</em></p>
 




    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2024 COLM Organization Committee</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>