[{"UID":"1","abstract":"TBD","bio":"Christopher Manning is the inaugural Thomas M. Siebel Professor of Machine Learning in the Departments of Linguistics and Computer Science at Stanford University, Director of the Stanford Artificial Intelligence Laboratory (SAIL), and an Associate Director of the Stanford Institute for Human-Centered Artificial Intelligence (HAI). His research goal is computers that can intelligently process, understand, and generate human languages. Manning was an early leader in applying Deep Learning to Natural Language Processing (NLP), with well-known research on the GloVe model of word vectors, attention, machine translation, question answering, self-supervised model pre-training, tree-recursive neural networks, machine reasoning, dependency parsing, sentiment analysis, and summarization. Manning has authored leading textbooks on statistical approaches to NLP and information retrieval, linguistic monographs on ergativity and complex predicates, and the CS224N Natural Language Processing with Deep Learning course, available on YouTube. He is an ACM Fellow, a AAAI Fellow, and an ACL Fellow, and a Past President of the ACL (2015). His research has won ACL, Coling, EMNLP, and CHI Best Paper Awards, an ACL Test of Time Award, and the IEEE John von Neumann Medal (2024). He is the founder of the Stanford NLP group (@stanfordnlp) and manages development of the Stanford CoreNLP and Stanza software.","image":"/static/images/speakers/manning.jpg","institution":"Stanford University","session":"TDB","speaker":"Christopher Manning","title":"TBD"},{"UID":"2","abstract":"Large language models have opened up new scientific opportunities in a variety of fields. Some challenges that had been core roadblocks in natural language processing for decades \u2013 such as inferring rich representations for language understanding and generating fluent and complex text \u2013 have now mostly been overcome. An exciting consequence of this is that we can now dive into more nuanced problems within the language sciences. Two frontiers in the computational modelling of language use \u2013 which we should arguably be better equipped to tackle now \u2013 concern perceptual grounding and social interaction. In this talk, I will review some of the current challenges of language models regarding these frontiers, present some recent work on multimodal and conversational grounding, and argue that there is still substantial progress to be made and plenty of interesting open questions ahead of us.","bio":"Raquel Fern\u00e1ndez is Full Professor of Computational Linguistics and Dialogue Systems at the Institute for Logic, Language & Computation, University of Amsterdam, where she leads the Dialogue Modelling Group. Her work and interests revolve around language use in context, including computational semantics and pragmatics, dialogue interaction, visually-grounded language processing, and language learning, among others. Her group carries out research on these topics at the interface of computational linguistics, cognitive science, and artificial intelligence. Raquel studied language and cognitive science in Barcelona, her home city, and received her PhD in computational linguistics from King's College London. Before moving to Amsterdam, she held research positions at the Linguistics Department of the University of Potsdam and at the Center for the Study of Language and Information (CSLI), Stanford University. Over the course of her career, she has been awarded several prestigious personal fellowships by the Dutch Research Council and is the recipient of a European Research Council (ERC) Consolidator Grant.","image":"/static/images/speakers/raquel.png","institution":"University of Amsterdam","session":"TDB","speaker":"Raquel Fern\u00e1ndez","title":"Multimodal and Conversational Grounding in the Era of LLMs"},{"UID":"3","abstract":"TBD","bio":"Dr. Fedorenko is a cognitive neuroscientist who studies the human language system and its relationship with other systems in the brain. She received her Bachelor\u2019s degree from Harvard University in 2002, and her Ph.D. from MIT in 2007. She was then awarded a K99R00 Pathway to Independence Career Development Award from NIH. In 2014, she joined the faculty at MGH/HMS, and in 2019 she returned to MIT where she is currently an Associate Professor in the Department of Brain and Cognitive Sciences and a member of the McGovern Institute for Brain Research. Dr. Fedorenko uses fMRI, intracranial recordings and stimulation, EEG, MEG, and computational modeling, to study adults and children, including individuals with developmental and acquired brain disorders, and individuals with structurally atypical brains, but typical-like cognition.","image":"/static/images/speakers/ev.png","institution":"MIT","session":"TDB","speaker":"Evelina Fedorenko","title":"TBD"},{"UID":"4","abstract":"TBD","bio":"Hanna Hajishirzi is the Torode Family Associate Professor in the Allen School of Computer Science and Engineering at the University of Washington and a Senior Director of NLP at AI2. Her current research delves into various domains within Natural Language Processing (NLP) and Artificial Intelligence (AI), with a particular emphasis on accelerating the science of language modeling, broadening their scope, and enhancing their applicability and usefulness for human lives. She has published over 140 scientific articles in prestigious journals and conferences across ML, AI, NLP, and Computer Vision. She is the recipient of numerous awards, including the Sloan Fellowship, NSF CAREER Award, Intel Rising Star Award, Allen Distinguished Investigator Award, Academic Achievement UIUC Alumni Award, and was a finalist for the Innovator of the Year Award by GeekWire. The work from her lab has been nominated for or has received best paper awards at various conferences and has been featured in numerous magazines and newspapers.","image":"/static/images/speakers/hanna.jpg","institution":"University of Washington / AI2","session":"TDB","speaker":"Hannaneh Hajishirzi","title":"TBD"}]
