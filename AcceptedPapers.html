


<!DOCTYPE html>
<html lang="en">
<head>
    
    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script> <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>


    <!-- Library libs_ext -->
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>
    <link rel="icon" href="/favicon.svg"/>
    <title>COLM 2024: Accepted Papers</title>
    
</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/logo.jpg"
                    height="auto"
            width="180px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
              
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="index.html" 
                      
                      >Home</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown1">
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="dates.html" 
                      
                      >Dates</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown2">
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link dropdown-toggle" href="#" 
                      
                      id="navbarDropdown3" role="button" data-toggle="dropdown"
                      
                      >Program</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown3">
                      
                      <a class="dropdown-item" href="Keynotes.html">Keynote Speakers</a>
                      
                      <a class="dropdown-item" href="AcceptedPapers.html">Accepted Papers</a>
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="cfp.html" 
                      
                      >Call for Papers</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown4">
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="AreaChairs.html" 
                      
                      >Area Chairs</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown5">
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link dropdown-toggle" href="#" 
                      
                      id="navbarDropdown6" role="button" data-toggle="dropdown"
                      
                      >Guides</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown6">
                      
                      <a class="dropdown-item" href="CoC.html">Code of Conduct</a>
                      
                      <a class="dropdown-item" href="CoE.html">Code of Ethics</a>
                      
                      <a class="dropdown-item" href="ReviewGuide.html">Review Guidelines</a>
                      
                      <a class="dropdown-item" href="AuthorGuide.html">Author Guide</a>
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="sponsors.html" 
                      
                      >Sponsors</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown7">
                      
                    </div>
                </li>
                
                <li class="nav-item dropdown  ">
                    <a class="nav-link " href="faq.html" 
                      
                      >FAQ</a>
                    <div class="dropdown-menu" aria-labelledby="navbarDropdown8">
                      
                    </div>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        



<h2>
    Accepted Papers
</h2>

<br />

<strong>This is a temporary list, just to get the information out (and get your excited about COLM üòÅ). We are aware of some lingering issues in author ordering. This will be fixed in the final release of the list accepted papers, after the camera ready deadline. Please don't use this information for citation purposes.</strong>

<br />
<br />


<p>A Survey on Deep Learning for Theorem Proving <br/> <em>Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, Xujie Si</em></p>
 

<p>Towards Measuring the Representation of Subjective Global Opinions in Language Models <br/> <em>Esin DURMUS, Karina Nguyen, Thomas Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, Deep Ganguli</em></p>
 

<p>Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM <br/> <em>Chunqiu Steven Xia, Yinlin Deng, LINGMING ZHANG</em></p>
 

<p>Transformer Circuit Evaluation Metrics Are Not Robust <br/> <em>Joseph Miller, Bilal Chughtai, William Saunders</em></p>
 

<p>Long-form Answers to Visual Questions Asked by Blind and Low Vision People <br/> <em>Mina Huh, Fangyuan Xu, Yi-Hao Peng, Chongyan Chen, Hansika Murugu, Danna Gurari, Eunsol Choi, Amy Pavel</em></p>
 

<p>Locating and Editing Factual Associations in Mamba <br/> <em>Arnab Sen Sharma, David Atkinson, David Bau</em></p>
 

<p>Does Incomplete Syntax Influence Korean Language Model? Focusing on Word Order and Case Markers <br/> <em>Jong Myoung Kim, Young-Jun Lee, Yong-Jin Han, Ho-Jin Choi, Sangkeun Jung</em></p>
 

<p>LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play <br/> <em>Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-yi Lee, Shao-Hua Sun</em></p>
 

<p>Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided <br/> <em>Hongli Zhan, Allen Zheng, Yoon Kyung Lee, Jina Suh, Junyi Jessy Li, Desmond Ong</em></p>
 

<p>STEMA: Scalable Toolkit for Efficient Model Alignment <br/> <em>Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel Egert, Shengyang Sun, Jimmy J. Zhang, Sahil Jain, Ali Taghibakhshi, Markel Sanz Ausin, Ashwath Aithal, Oleksii Kuchaiev</em></p>
 

<p>Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language? <br/> <em>Omid Ghahroodi, Marzia Nouri, Mohammad Vali Sanian, Alireza Sahebi, Doratossadat Dastgheib, Ehsaneddin Asgari, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban</em></p>
 

<p>How Susceptible are LLMs to Influence in Prompts? <br/> <em>Sotiris Anagnostidis, Jannis Bulian</em></p>
 

<p>PairEval: Open-domain Dialogue Evaluation Metric with Pairwise Comparisons <br/> <em>ChaeHun Park, Minseok Choi, Dohyun Lee, Jaegul Choo</em></p>
 

<p>HGRN2: Gated Linear RNNs with State Expansion <br/> <em>Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong</em></p>
 

<p>Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts <br/> <em>Evgenii Kortukov, Alexander Rubinstein, Elisa Nguyen, Seong Joon Oh</em></p>
 

<p>Investigating Instruction Tuning Large Language Models on Graphs <br/> <em>Kerui Zhu, Bo-Wei Huang, Bowen Jin, Yizhu Jiao, Ming Zhong, Kevin Chang, Shou-De Lin, Jiawei Han</em></p>
 

<p>FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers <br/> <em>Joshua Nathaniel Williams, J Zico Kolter</em></p>
 

<p>CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization <br/> <em>Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, Peter Clark</em></p>
 

<p>Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game <br/> <em>Silin Du, Xiaowei Zhang</em></p>
 

<p>Factual and Tailored Recommendation Endorsements using Language Models and Reinforcement Learning <br/> <em>Jihwan Jeong, Yinlam Chow, Guy Tennenholtz, ChihWei Hsu, Mohammad Ghavamzadeh, Craig Boutilier</em></p>
 

<p>How Well Do LLMs Identify Cultural Unity in Diversity? <br/> <em>Jialin Li, Junli Wang, Junjie Hu, Ming Jiang</em></p>
 

<p>Guiding Language Model Math Reasoning with Planning Tokens <br/> <em>Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, Alessandro Sordoni</em></p>
 

<p>Dated Data: Tracing Knowledge Cutoffs in Large Language Models <br/> <em>Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme</em></p>
 

<p>Large Language Model is not a (Multilingual) Compositional Relation Reasoner <br/> <em>Jinman Zhao, Xueyan Zhang</em></p>
 

<p>Instruction Mining: Instruction Data Selection for Tuning Large Language Models <br/> <em>Yihan Cao, Yanbin Kang, Chi Wang, Lichao Sun</em></p>
 

<p>Does your data spark joy? Performance gains from domain upsampling at the end of training <br/> <em>Cody Blakeney, Mansheej Paul, Brett W. Larsen, Sean Owen, Jonathan Frankle</em></p>
 

<p>Predicting Emergent Capabilities by Finetuning <br/> <em>Charlie Victor Snell, Eric Wallace, Dan Klein, Sergey Levine</em></p>
 

<p>Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data <br/> <em>Tim Baumg√§rtner, Yang Gao, Dana Alon, Donald Metzler</em></p>
 

<p>CATS: Context-Aware Thresholding for Sparsity in Large Language Models <br/> <em>Donghyun Lee, Jaeyong Lee, Genghan Zhang, Mo Tiwari, Azalia Mirhoseini</em></p>
 

<p>Efficient Hybrid Long Sequence Modeling with State Space Augmented Transformers <br/> <em>Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis X Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao</em></p>
 

<p>Does Collaborative Human‚ÄìLM Dialogue Generation Help Information Extraction from Human‚ÄìHuman Dialogues? <br/> <em>Bo-Ru Lu, Nikita Haduong, Chia-Hsuan Lee, Zeqiu Wu, Hao Cheng, Paul Koester, Jean Utke, Tao Yu, Noah A. Smith, Mari Ostendorf</em></p>
 

<p>Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens <br/> <em>Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh Hajishirzi</em></p>
 

<p>RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation <br/> <em>Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, Jie Fu</em></p>
 

<p>Exploring the Mystery of Influential Data for Mathematical Reasoning <br/> <em>Xinzhe Ni, Yeyun Gong, Zhibin Gou, yelong shen, Yujiu Yang, Nan Duan, Weizhu Chen</em></p>
 

<p>LalaEval: A Holistic Human Evaluation Framework for Domain-Specific Large Language Models <br/> <em>Chongyan Sun, Ken Lin, Shiwei Wang, Hulong Wu, Chengfei Fu, Zhen Wang</em></p>
 

<p>Trust No Bot? Personal Disclosures in Human-LLM Conversations <br/> <em>Yash More, Niloofar Mireshghallah, Maria Antoniak, Yejin Choi, Golnoosh Farnadi</em></p>
 

<p>Mamba: Linear-Time Sequence Modeling with Selective State Spaces <br/> <em>Albert Gu, Tri Dao</em></p>
 

<p>MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries <br/> <em>Yixuan Tang, Yi Yang</em></p>
 

<p>How bad is training on synthetic data? A statistical analysis of language model collapse <br/> <em>Mohamed El Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre Youssef, Merouane Abdelkader DEBBAH</em></p>
 

<p>V-STaR: Training Verifiers for Self-Taught Reasoners <br/> <em>Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal</em></p>
 

<p>Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence <br/> <em>Bo Peng, Daniel Goldstein, Quentin Gregory Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Kranthi Kiran GV, Haowen Hou, Satyapriya Krishna, Ronald McClelland Jr., Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Jian Zhu, Rui-Jie Zhu</em></p>
 

<p>Linearizing Large Language Models <br/> <em>Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar</em></p>
 

<p>VideoDirectorGPT: Consistent Multi-Scene Video Generation via LLM-Guided Planning <br/> <em>Han Lin, Abhay Zala, Jaemin Cho, Mohit Bansal</em></p>
 

<p>OpenAgents: An Open Platform for Language Agents in the Wild <br/> <em>Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Zeyu Leo Liu, Yiheng Xu, Hongjin SU, Dongchan Shin, Caiming Xiong, Tao Yu</em></p>
 

<p>TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance <br/> <em>Haorui Wang, Rongzhi Zhang, Yinghao Li, Lingkai Kong, Yuchen Zhuang, Xiusi Chen, Chao Zhang</em></p>
 

<p>Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP <br/> <em>Fran√ßois Remy, Pieter Delobelle, Hayastan Avetisyan, Alfiya Khabibullina, Miryam de Lhoneux, Thomas Demeester</em></p>
 

<p>RAFT: Adapting Language Model to Domain Specific RAG <br/> <em>Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, Joseph E. Gonzalez</em></p>
 

<p>PhonATe: Impact of Type-Written Phonological Features of African American Language on Generative Language Modeling Tasks <br/> <em>Nicholas Deas, Jessica A Grieser, Xinmeng Hou, Shana Kleiner, Tajh Martin, Sreya Nandanampati, Desmond U. Patton, Kathleen McKeown</em></p>
 

<p>On the Geometry of Context and Word Embeddings in Next-Token-Prediction <br/> <em>Yize Zhao, Tina Behnia, Vala Vakilian, Christos Thrampoulidis</em></p>
 

<p>Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think <br/> <em>Xinpeng Wang, Chengzhi Hu, Bolei Ma, Paul R√∂ttger, Barbara Plank</em></p>
 

<p>ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs <br/> <em>Pengrui Han, Rafal Dariusz Kocielnik, Adhithya Prakash Saravanan, Roy Luoyao Jiang, Or Sharir, Anima Anandkumar</em></p>
 

<p>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation <br/> <em>Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Hu Jinfang, Bowen Zhou</em></p>
 

<p>Resolving Knowledge Conflicts in Large Language Models <br/> <em>Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov</em></p>
 

<p>How Far Are We from Intelligent Visual Deductive Reasoning? <br/> <em>Yizhe Zhang, Richard He Bai, Ruixiang ZHANG, Jiatao Gu, Shuangfei Zhai, Joshua M. Susskind, Navdeep Jaitly</em></p>
 

<p>DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training <br/> <em>Dacheng Li, Rulin Shao, Anze Xie, Eric Xing, Xuezhe Ma, Ion Stoica, Joseph E. Gonzalez, Hao Zhang</em></p>
 

<p>Web Retrieval Agents for Evidence-Based Misinformation Detection <br/> <em>Jacob-Junqi Tian, Hao Yu, Yury Orlovskiy, Tyler Vergho, Mauricio Rivera, Mayank Goel, Zachary Yang, Jean-Fran√ßois Godbout, Reihaneh Rabbany, Kellin Pelrine</em></p>
 

<p>&#34;Task Success&#34; is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors <br/> <em>Lin Guan, Yifan Zhou, Denis Liu, Yantian Zha, Heni Ben Amor, Subbarao Kambhampati</em></p>
 

<p>Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images <br/> <em>Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu</em></p>
 

<p>PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models <br/> <em>Devansh Jain, Priyanshu Kumar, Samuel Gehman, Xuhui Zhou, Thomas Hartvigsen, Maarten Sap</em></p>
 

<p>Reasoning about concepts with LLMs: Inconsistencies abound <br/> <em>Rosario Uceda Sosa, Karthikeyan Natesan Ramamurthy, Maria Chang, Moninder Singh</em></p>
 

<p>Logits of API-Protected LLMs Leak Proprietary Information <br/> <em>Matthew Finlayson, Xiang Ren, Swabha Swayamdipta</em></p>
 

<p>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking <br/> <em>Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah Goodman</em></p>
 

<p>Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM <br/> <em>Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Roziere, Jacob Kahn, Shang-Wen Li, Wen-tau Yih, Jason E Weston, Xian Li</em></p>
 

<p>AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts <br/> <em>Zefang Liu, Jiahua Luo</em></p>
 

<p>Instruction-tuning Aligns LLMs to the Human Brain <br/> <em>Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, Antoine Bosselut</em></p>
 

<p>An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models <br/> <em>Emmy Liu, Graham Neubig, Jacob Andreas</em></p>
 

<p>Learning to Plan for Language Modeling from Unlabeled Data <br/> <em>Nathan Cornille, Marie-Francine Moens, Florian Mai</em></p>
 

<p>Impact of Preference Noise on the Alignment Performance of Generative Language Models <br/> <em>Yang Gao, Dana Alon, Donald Metzler</em></p>
 

<p>SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models <br/> <em>Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng ZHANG, Dahua Lin</em></p>
 

<p>Eliciting Latent Knowledge from &#34;Quirky&#34; Language Models <br/> <em>Alex Troy Mallen, Madeline Brumley, Julia Kharchenko, Nora Belrose</em></p>
 

<p>AmbigDocs: Reasoning across Documents on Different Entities under the Same Name <br/> <em>Yoonsang Lee, Xi Ye, Eunsol Choi</em></p>
 

<p>Is ChatGPT a Good Sentiment Analyzer? <br/> <em>Zengzhi Wang, Qiming Xie, Yi Feng, Zixiang Ding, Zinong Yang, Rui Xia</em></p>
 

<p>Multi-FAct for Multi-lingual Factuality Evaluation <br/> <em>Sheikh Shafayat, Eunsu Kim, Juhyun Oh, Alice Oh</em></p>
 

<p>Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models <br/> <em>Sicheng Zhu, Bang An, Ruiyi Zhang, Michael-Andrei Panaitescu-Liess, Yuancheng Xu, Furong Huang</em></p>
 

<p>LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset <br/> <em>Botao Yu, Frazier N. Baker, Ziqi Chen, Xia Ning, Huan Sun</em></p>
 

<p>Evaluating In-context Conversational Adaptation in Multimodal LLMs <br/> <em>Yilun Hua, Yoav Artzi</em></p>
 

<p>Rejection Improves Reliability: Training LLMs to Refuse Un-known Questions Using RL from Knowledge Feedback <br/> <em>Hongshen Xu, Zichen Zhu, Situo Zhang, Da Ma, Shuai Fan, Lu Chen, Kai Yu</em></p>
 

<p>Boundary detection in mixed AI-human texts <br/> <em>Laida Kushnareva, Tatiana Gaintseva, Dmitry Abulkhanov, Kristian Kuznetsov, German Magai, Eduard Tulchinskii, Serguei Barannikov, Sergey Nikolenko, Irina Piontkovskaya</em></p>
 

<p>CA-LoRA: Adapting Existing LoRA for Compressed LLMs to Enable Efficient Multi-Tasking on Personal Devices <br/> <em>Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, Kuai Li, Chen Chen, TAO YANG, Maosong Sun</em></p>
 

<p>Don&#39;t throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding <br/> <em>Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz</em></p>
 

<p>Crystal: Illuminating LLM Abilities on Language and Code <br/> <em>Tianhua Tao, Junbo Li, Bowen Tan, Hongyi Wang, William Marshall, Bhargav M Kanakiya, Joel Hestness, Natalia Vassilieva, Zhiqiang Shen, Eric Xing, Zhengzhong Liu</em></p>
 

<p>GeniL: A Multilingual Dataset on Generalizing Language <br/> <em>Aida Mostafazadeh Davani, Sagar Gubbi Venkatesh, Sunipa Dev, Shachi Dave, Vinodkumar Prabhakaran</em></p>
 

<p>RULER: What‚Äôs the Real Context Size of Your Long-Context Language Models? <br/> <em>Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Boris Ginsburg</em></p>
 

<p>The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization <br/> <em>Shengyi Huang, Michael Noukhovitch, Arian Hosseini, Kashif Rasul, Weixun Wang, Lewis Tunstall</em></p>
 

<p>Can Language Models Solve Olympiad Programming? <br/> <em>Quan Shi, Michael Tang, Karthik R Narasimhan, Shunyu Yao</em></p>
 

<p>From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function <br/> <em>Rafael Rafailov, Joey Hejna, Ryan Park, Chelsea Finn</em></p>
 

<p>PRobELM: Plausibility Ranking Evaluation for Language Models <br/> <em>Moy Yuan, Chenxi Whitehouse, Eric Chamoun, Rami Aly, Andreas Vlachos</em></p>
 

<p>Bring Your Own Data! Self-Sensitivity Evaluation for Large Language Models <br/> <em>Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein</em></p>
 

<p>Can MLLMs Perform Text-to-Image In-Context Learning? <br/> <em>Yuchen Zeng, Wonjun Kang, Yicong Chen, Hyung Il Koo, Kangwook Lee</em></p>
 

<p>DeStein: Navigating Detoxification of Language Models via Universal Steering Pairs and Head-wise Activation Fusion <br/> <em>Yu Li, Zhihua Wei, Han Jiang, Chuanyang Gong</em></p>
 

<p>How Do In-Context Documents Affect Long-form Answer Generation? <br/> <em>Hung-Ting Chen, Fangyuan Xu, Shane Arora, Eunsol Choi</em></p>
 

<p>LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal Classification <br/> <em>Zhen Qin, Junru Wu, Jiaming Shen, Tianqi Liu, Xuanhui Wang</em></p>
 

<p>LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models <br/> <em>Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei</em></p>
 

<p>Do Large Language Models Have Compositional Ability? An Investigation into Scalability and Limitations <br/> <em>Zhuoyan Xu, Zhenmei Shi, Yingyu Liang</em></p>
 

<p>Decomposing Label Space, Format and Discrimination: Re- thinking How LLMs Respond and Solve Tasks via In-Context Learning <br/> <em>Quanyu Long, Yin Wu, Wenya Wang, Sinno Jialin Pan</em></p>
 

<p>Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports <br/> <em>Tianyu Cao, Natraj Raman, Danial Dervovic, Chenhao Tan</em></p>
 

<p>NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness <br/> <em>Manav Singhal, Tushar Aggarwal, Abhijeet Awasthi, Nagarajan Natarajan, Aditya Kanade</em></p>
 

<p>TarGEN: Targeted Data Generation with Large Language Models <br/> <em>Himanshu Gupta, Kevin Scaria, Ujjwala Anantheswaran, Shreyas Verma, Mihir Parmar, Saurabh Arjun Sawant, Chitta Baral, Swaroop Mishra</em></p>
 

<p>Uncovering Intermediate Variables in Transformers using Circuit Probing <br/> <em>Michael A. Lepori, Thomas Serre, Ellie Pavlick</em></p>
 

<p>UniMem: Towards a Unified View of Long-Context Large Language Models <br/> <em>Junjie Fang, Likai Tang, Hongzhe Bi, Yujia Qin, Si Sun, Zhenyu Li, Haolun Li, Yongjian Li, Xin Cong, Yankai Lin, Yukun Yan, Xiaodong Shi, Sen Song, Zhiyuan Liu, Maosong Sun</em></p>
 

<p>Towards Verifiable Text Generation with Symbolic References <br/> <em>Lucas Torroba Hennigen, Zejiang Shen, Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Yoon Kim</em></p>
 

<p>WebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? <br/> <em>Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, Xiang Yue</em></p>
 

<p>HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs <br/> <em>Junying Chen, Xidong Wang, Ke Ji, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Song Dingjie, Wenya Xie, Chuyi Kong, Jianquan Li, Xiang Wan, Haizhou Li, Benyou Wang</em></p>
 

<p>LMD3: Language Model Data Density Dependence <br/> <em>John Kirchenbauer, Garrett Honke, Gowthami Somepalli, Jonas Geiping, Katherine Lee, Daphne Ippolito, Tom Goldstein, David Andre</em></p>
 

<p>The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models <br/> <em>Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, Jay Pujara</em></p>
 

<p>Tuning Language Models by Proxy <br/> <em>Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, Noah A. Smith</em></p>
 

<p>Evaluating LLMs at Detecting Errors in LLM Responses <br/> <em>Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Haoran Ranran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui Zhang</em></p>
 

<p>HDT: Hierarchical Document Transformer <br/> <em>Haoyu He, Markus Flicke, Jan Buchmann, Iryna Gurevych, Andreas Geiger</em></p>
 

<p>With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation <br/> <em>Yan Wang, Dongyang Ma, Deng Cai</em></p>
 

<p>CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text <br/> <em>Zhenru Lin, Yiqun Yao, Yang Yuan</em></p>
 

<p>Learning From Correctness Without Prompting Makes LLM Efficient Reasoner <br/> <em>Yuxuan Yao, Han Wu, Zhijiang Guo, Zhou Biyan, Jiahui Gao, Sichun Luo, Hanxu Hou, Xiaojin Fu, Linqi Song</em></p>
 

<p>Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph <br/> <em>Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini</em></p>
 

<p>Scalable Model Editing via Customized Expert Networks <br/> <em>zihan yao, Yu He, Tianyu Qi, Ming Li</em></p>
 

<p>Fine-grained Hallucination Detection and Editing for Language Models <br/> <em>Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi</em></p>
 

<p>Ontology-Guided Retrieval-Augmented Generation for Theme-Specific Entity Typing <br/> <em>Jinfeng Xiao, Linyi Ding, James Barry, Mohab Elkaref, Geeth De Mel, Jiawei Han</em></p>
 

<p>Unified View of Grokking, Double Descent and Emergent Abilities: A Comprehensive Study on Algorithm Task <br/> <em>Yufei Huang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun</em></p>
 

<p>Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations <br/> <em>Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, Dongwon Lee</em></p>
 

<p>Personalized Collaborative Fine-Tuning for On-Device Large Language Models <br/> <em>Nicolas Wagner, Dongyang Fan, Martin Jaggi</em></p>
 

<p>Benchmarks as Microscopes: A Call for Model Metrology <br/> <em>Michael Saxon, Naomi Saphra, Ari Holtzman, Peter West, William Yang Wang</em></p>
 

<p>Tabular Transfer Learning via Prompting LLMs <br/> <em>Jaehyun Nam, Woomin Song, Seong Hyeon Park, Jihoon Tack, Sukmin Yun, Jaehyung Kim, Kyu Hwan Oh, Jinwoo Shin</em></p>
 

<p>How Multilingual are Large Language Models Fine-tuned for Translation? <br/> <em>Aquia Richburg, Marine Carpuat</em></p>
 

<p>O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models <br/> <em>Yuchen Xiao, Yanchao Sun, Mengda Xu, Udari Madhushani Sehwag, Jared Vann, Deepeka Garg, Sumitra Ganesh</em></p>
 

<p>LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models <br/> <em>Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, Zhiting Hu</em></p>
 

<p>Do Membership Inference Attacks Work on Large Language Models? <br/> <em>Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, Hannaneh Hajishirzi</em></p>
 

<p>Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics <br/> <em>James A. Michaelov, Catherine Arnett, Ben Bergen</em></p>
 

<p>The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets <br/> <em>Samuel Marks, Max Tegmark</em></p>
 

<p>Hummer: Towards Limited Competitive Preference Dataset <br/> <em>Yusen Wu, Li Jiang, Junwu Xiong, Jingqing Ruan, Yichuan Ding, Qingpei Guo, zujie wen, JUN ZHOU, Xiaotie Deng</em></p>
 

<p>Zephyr: Direct Distillation of LM Alignment <br/> <em>Lewis Tunstall, Edward Emanuel Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Cl√©mentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M Rush, Thomas Wolf</em></p>
 

<p>Nonparametric Variational Regularisation of Pretrained Transformers <br/> <em>Fabio James Fehr, James Henderson</em></p>
 

<p>Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability <br/> <em>Jiri Hron, Laura A Culp, Gamaleldin Fathy Elsayed, Rosanne Liu, Jasper Snoek, Simon Kornblith, Alex Rizkowsky, Isabelle Simpson, Jascha Sohl-Dickstein, Noah Fiedel, Aaron T Parisi, Alexander A Alemi, Azade Nova, Ben Adlam, Bernd Bohnet, Gaurav Mishra, Hanie Sedghi, Izzeddin Gur, Jaehoon Lee, John D Co-Reyes, Kathleen Kenealy, Kelvin Xu, Kevin Swersky, Igor Mordatch, Lechao Xiao, Maxwell Bileschi, Peter J Liu, Roman Novak, Sharad Vikram, Tris Warkentin, Jeffrey Pennington</em></p>
 

<p>Redesigning Information Markets in the Era of Language Models <br/> <em>Martin Weiss, Nasim Rahaman, Manuel Wuthrich, Yoshua Bengio, Li Erran Li, Bernhard Sch√∂lkopf, Christopher Pal</em></p>
 

<p>Large Language Model Routing with Benchmark Datasets <br/> <em>Tal Shnitzer, Anthony Ou, M√≠rian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, Mikhail Yurochkin</em></p>
 

<p>Language Models as Critical Thinking Tools: A Case Study of Philosophers <br/> <em>Andre Ye, Jared Moore, Rose Novick, Amy X Zhang</em></p>
 

<p>Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates <br/> <em>Avanika Narayan, Mayee F Chen, Kush Bhatia, Christopher Re</em></p>
 

<p>Prompt Exploration with Prompt Regression <br/> <em>Michael Feffer, Ronald Xu, Yuekai Sun, Mikhail Yurochkin</em></p>
 

<p>Evaluating faithfulness and content selection in book-length summarization <br/> <em>Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, Mohit Iyyer</em></p>
 

<p>Mapping the Increasing Use of LLMs in Scientific Papers <br/> <em>Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D Manning, James Y. Zou</em></p>
 

<p>Scattered Mixture-of-Experts Implementation <br/> <em>Shawn Tan, Yikang Shen, Rameswar Panda, Aaron Courville</em></p>
 

<p>What Are Tools Anyway? A Survey from the Language Model Perspective <br/> <em>Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, Graham Neubig</em></p>
 

<p>A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration <br/> <em>Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang</em></p>
 

<p>Data Checklist: On Unit-Testing Datasets with Usable Information <br/> <em>Heidi Chenyu Zhang, Shabnam Behzad, Kawin Ethayarajh, Dan Jurafsky</em></p>
 

<p>MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs <br/> <em>Vera Neplenbroek, Arianna Bisazza, Raquel Fern√°ndez</em></p>
 

<p>MambaByte: Token-free Selective State Space Model <br/> <em>Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M Rush</em></p>
 

<p>Description-Based Text Similarity <br/> <em>Shauli Ravfogel, Valentina Pyatkin, Amir David Nissan Cohen, Avshalom Manevich, Yoav Goldberg</em></p>
 

<p>Generating Synthetic Datasets for Few-shot Prompt Tuning <br/> <em>Xu Guo, Zilin Du, Boyang Li, Chunyan Miao</em></p>
 

<p>Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in Subjective Tasks? <br/> <em>Urja Khurana, Eric Nalisnick, Antske Fokkens, Swabha Swayamdipta</em></p>
 

<p>Does RoBERTa Perform Better than BERT in Continual Learning: An Attention Sink Perspective <br/> <em>Xueying Bai, Yifan Sun, Niranjan Balasubramanian</em></p>
 

<p>An In-Context Learning Agent for Formal Theorem-Proving <br/> <em>Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, Swarat Chaudhuri</em></p>
 

<p>Efficient Parallelization Layouts for Large-Scale Distributed Model Training <br/> <em>Johannes Hagemann, Samuel Weinbach, Konstantin Dobler, Maximilian Schall, Gerard de Melo</em></p>
 

<p>Unforgettable Generalization in Language Models <br/> <em>Eric Zhang, Leshem Choshen, Jacob Andreas</em></p>
 

<p>MileBench: Benchmarking MLLMs in Long Context <br/> <em>Song Dingjie, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, Benyou Wang</em></p>
 

<p>AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs <br/> <em>Zeyi Liao, Huan Sun</em></p>
 

<p>List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs <br/> <em>An Yan, Zhengyuan Yang, Junda Wu, Wanrong Zhu, Jianwei Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Julian McAuley, Jianfeng Gao, Lijuan Wang</em></p>
 

<p>Source-Aware Training Enables Knowledge Attribution in Language Models <br/> <em>Muhammad Khalifa, David Wadden, Emma Strubell, Honglak Lee, Lu Wang, Iz Beltagy, Hao Peng</em></p>
 

<p>A Language Agent for Autonomous Driving <br/> <em>Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, Yue Wang</em></p>
 

<p>Auxiliary task demands mask the capabilities of smaller language models <br/> <em>Jennifer Hu, Michael Frank</em></p>
 

<p>LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition <br/> <em>Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, Min Lin</em></p>
 

<p>GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark <br/> <em>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman</em></p>
 

<p>Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms <br/> <em>Michael Hanna, Sandro Pezzelle, Yonatan Belinkov</em></p>
 

<p>Stronger Random Baselines for In-Context Learning <br/> <em>Gregory Yauney, David Mimno</em></p>
 

<p>Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities <br/> <em>Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio Yokota, Naoaki Okazaki</em></p>
 

<p>Decoupling Noise and Toxic Parameters for Language Model Detoxification by Task Vector Merging <br/> <em>Yongmin Kim, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo</em></p>
 

<p>Optimising Calls to Large Language Models with Uncertainty-Based Two-Tier Selection <br/> <em>Guillem Ram√≠rez, Alexandra Birch, Ivan Titov</em></p>
 

<p>Adaptive Quantization Error Reconstruction for LLMs with Mixed Precision <br/> <em>Lin Ou, Jinpeng Xia, Yuewei Zhang, Chuzhan Hao, Hao Henry Wang</em></p>
 

<p>StyleTalker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation <br/> <em>Yinghao Aaron Li, Xilin Jiang, Jordan Darefsky, Ge Zhu, Nima Mesgarani</em></p>
 

<p>Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images <br/> <em>Ali Naseh, Katherine Thai, Mohit Iyyer, Amir Houmansadr</em></p>
 

<p>Compression Represents Intelligence Linearly <br/> <em>Yuzhen Huang, Jinghan Zhang, Zifei Shan, Junxian He</em></p>
 

<p>Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping <br/> <em>Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, Yuandong Tian</em></p>
 

<p>How Easily do Irrelevant Inputs Skew the Responses of Large Language Models? <br/> <em>Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao</em></p>
 

<p>Embedding Nationality Context into LLM-Based Human Response Simulations <br/> <em>Louis Kwok, Michal Bravansky, Lewis Griffin</em></p>
 

<p>Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning <br/> <em>Tinghui Zhu, Kai Zhang, Jian Xie, Yu Su</em></p>
 

<p>LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory <br/> <em>Jillian Ross, Yoon Kim, Andrew Lo</em></p>
 

<p>CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias <br/> <em>Vipul Gupta, Pranav Narayanan Venkit, Hugo Lauren√ßon, Shomir Wilson, Rebecca J. Passonneau</em></p>
 

<p>Chinese Tiny LLM: Pretraining a Chinese-Centered Large Language Model <br/> <em>Xeron Du, Zhouliang Yu, Songyang Gao, Ding Pan, Cheng Yuyang, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Wenhu Chen, Ge Zhang</em></p>
 

<p>Using Natural Language Explanations to Rescale Human Judgments <br/> <em>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</em></p>
 

<p>LLM360: Towards Fully Transparent Open-Source LLMs <br/> <em>Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Timothy Baldwin, Eric Xing</em></p>
 

<p>From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsies <br/> <em>Shuxian Fan, Adam Visokay, Kentaro Hoffman, Stephen Salerno, Li Liu, Jeffrey T. Leek, Tyler McCormick</em></p>
 

<p>The Larger the Better? Improved LLM Code-Generation via Budget Reallocation <br/> <em>Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, Yossi Adi</em></p>
 

<p>&#34;Merge Conflicts!&#39;&#34; Exploring the Impacts of External Knowledge Distractors to Parametric Knowledge Graphs <br/> <em>Cheng Qian, Xinran Zhao, Tongshuang Wu</em></p>
 

<p>Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models <br/> <em>Adam Karvonen</em></p>
 

<p>AgentKit: Structured LLM Reasoning with Dynamic Graphs <br/> <em>Yue Wu, Yewen Fan, So Yeon Min, Shrimai Prabhumoye, Stephen Marcus McAleer, Russ Salakhutdinov, Yonatan Bisk, Yuanzhi Li, Tom Mitchell</em></p>
 

<p>A Reparameterized Discrete Diffusion Model for Text Generation <br/> <em>Lin Zheng, Jianbo Yuan, Lei Yu, Lingpeng Kong</em></p>
 

<p>Best Practices and Lessons Learned on Synthetic Data <br/> <em>Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, Andrew M. Dai</em></p>
 

<p>Let‚Äôs Think Dot by Dot: Hidden computation in transformer language models <br/> <em>Jacob Pfau, William Merrill, Samuel R. Bowman</em></p>
 

<p>Multi-hop Question Answering under Temporal Knowledge Editing <br/> <em>Keyuan Cheng, Gang Lin, Haoyang Fei, Yuxuan Zhai, Lu Yu, Muhammad Asif Ali, Lijie Hu, Di Wang</em></p>
 

<p>DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning <br/> <em>Abhay Zala, Han Lin, Jaemin Cho, Mohit Bansal</em></p>
 

<p>Autonomous Evaluation and Refinement of Digital Agents <br/> <em>Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, Alane Suhr</em></p>
 

<p>Building a Large Japanese Web Corpus for Large Language Models <br/> <em>Naoaki Okazaki, Kakeru Hattori, Hirai Shota, Hiroki Iida, Masanari Ohi, Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Rio Yokota, Sakae Mizuki</em></p>
 

<p>Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck <br/> <em>Nathan Godey, √âric Villemonte de la Clergerie, Beno√Æt Sagot</em></p>
 

<p>Are Language Models Robust Coreference Resolvers? <br/> <em>Nghia T. Le, Alan Ritter</em></p>
 

<p>Information Guided Regularization for Fine-tuning Language Models <br/> <em>Mandar Sharma, Nikhil Muralidhar, Shengzhe Xu, Raquib Bin Yousuf, Naren Ramakrishnan</em></p>
 

<p>Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning <br/> <em>Ruiqi Zhang, Licong Lin, Yu Bai, Song Mei</em></p>
 

<p>Generating Probabilistic Scenario Programs from Natural Language <br/> <em>Karim Elmaaroufi, Devan Shanker, Ana Cismaru, Marcell Vazquez-Chanlatte, Alberto Sangiovanni-Vincentelli, Matei Zaharia, Sanjit A. Seshia</em></p>
 

<p>Your Context Is Not an Array: Unveiling Random Access Limitations in Transformers <br/> <em>MohammadReza Ebrahimi, Sunny Panchal, Roland Memisevic</em></p>
 

<p>Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense? <br/> <em>Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, Dan Roth</em></p>
 

<p>From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples <br/> <em>Robert Vacareanu, Vlad Andrei Negru, Vasile Suciu, Mihai Surdeanu</em></p>
 

<p>Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey <br/> <em>Philipp Mondorf, Barbara Plank</em></p>
 

<p>Forklift: An Extensible Neural Lifter <br/> <em>Jordi Armengol-Estap√©, Rodrigo C. O. Rocha, Jackson Woodruff, Pasquale Minervini, Michael O&#39;Boyle</em></p>
 

<p>Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training <br/> <em>Zexuan Zhong, Mengzhou Xia, Danqi Chen, Mike Lewis</em></p>
 

<p>What makes a good metric? Meta-evaluating automatic metrics for text-to-image consistency <br/> <em>Candace Ross, Melissa Hall, Adriana Romero-Soriano, Adina Williams</em></p>
 

<p>Empowering Large Language Model Agents through Action Learning <br/> <em>Haiteng Zhao, Chang Ma, Guoyin Wang, Jing Su, Lingpeng Kong, Jingjing Xu, Zhi-Hong Deng, Hongxia Yang</em></p>
 

<p>On Limitations of the Transformer Architecture <br/> <em>Binghui Peng, Srini Narayanan, Christos Papadimitriou</em></p>
 

<p>IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations <br/> <em>Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, Willie Neiswanger</em></p>
 

<p>On Fairness of Low-Rank Adaptation of Large Models <br/> <em>Zhoujie Ding, Ken Liu, Pura Peetathawatchai, Berivan Isik, Sanmi Koyejo</em></p>
 

<p>Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning <br/> <em>Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Daogao Liu, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang</em></p>
 

<p>Information-Theoretic Distillation for Reference-less Summarization <br/> <em>Jaehun Jung, Ximing Lu, Liwei Jiang, Faeze Brahman, Peter West, Pang Wei Koh, Yejin Choi</em></p>
 

<p>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders <br/> <em>Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, Siva Reddy</em></p>
 

<p>Faithful and Unfaithful Error Recovery in Chain of Thought <br/> <em>Evelyn Yee, Alice Li, Chenyu Tang, Yeon Ho Jung, Ramamohan Paturi, Leon Bergen</em></p>
 

<p>Interpretable Gradient-Based Adversarial Attacks on Large Language Models <br/> <em>Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, Tong Sun</em></p>
 

<p>Beyond Correctness: Exercising Language Models for Efficient Code Generation <br/> <em>Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, LINGMING ZHANG</em></p>
 

<p>Early Weight Averaging meets High Learning Rates for LLM Pre-training <br/> <em>Sunny Sanyal, Atula Tejaswi, Jean Kaddour, Abhishek Kumar, Sujay Sanghavi</em></p>
 

<p>Chain-of-Symbol Prompting For Spatial Reasoning in Large Language Models <br/> <em>Hanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, Yue Zhang</em></p>
 

<p>What&#39;s in Your &#34;Safe&#34; Data?: Identifying Benign Data that Breaks Safety <br/> <em>Luxi He, Mengzhou Xia, Peter Henderson</em></p>
 

<p>TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding <br/> <em>Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, Beidi Chen</em></p>
 

<p>Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models <br/> <em>Sebastian Bordt, Harsha Nori, Vanessa Cristiny Rodrigues Vasconcelos, Besmira Nushi, Rich Caruana</em></p>
 

<p>Reverse Training to Nurse the Reversal Curse <br/> <em>Olga Golovneva, Zeyuan Allen-Zhu, Jason E Weston, Sainbayar Sukhbaatar</em></p>
 

<p>LLM4Causal: Large Language Model for Causal Decision Making <br/> <em>Haitao Jiang, Lin Ge, Yuhe Gao, Jianian Wang, Rui Song</em></p>
 

<p>Starling-7B: Improving Helpfulness and Harmlessness with RLAIF <br/> <em>Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Karthik Ganesan, Wei-Lin Chiang, Jian Zhang, Jiantao Jiao</em></p>
 

<p>RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models <br/> <em>Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chang, Bryan Catanzaro</em></p>
 

<p>JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks <br/> <em>Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, Chaowei Xiao</em></p>
 

<p>A Long Way to Go: Investigating Length Correlations in RLHF <br/> <em>Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett</em></p>
 

<p>Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers <br/> <em>Andy Yang, David Chiang</em></p>
 

<p>CoLLEGe: Concept Embedding Generation for Large Language Models <br/> <em>Ryan Teehan, Brenden M. Lake, Mengye Ren</em></p>
 

<p>CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration <br/> <em>Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing HONG, Lingpeng Kong, Xin Jiang, Zhenguo Li</em></p>
 

<p>Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding <br/> <em>Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, William Brandon</em></p>
 

<p>MONAL: Model Autophagy Analysis for Modeling Human-AI Interactions <br/> <em>Shu Yang, Muhammad Asif Ali, Lu Yu, Lijie Hu, Di Wang</em></p>
 

<p>EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents <br/> <em>Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal</em></p>
 

<p>Massive Activations in Large Language Models <br/> <em>Mingjie Sun, Xinlei Chen, J Zico Kolter, Zhuang Liu</em></p>
 

<p>Suspicion Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4 <br/> <em>Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, Yutaka Matsuo</em></p>
 

<p>Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models <br/> <em>Simon Chi Lok Yu, Jie He, Pasquale Minervini, Jeff Z. Pan</em></p>
 

<p>StructLM: Towards Building Generalist Models for Structured Knowledge Grounding <br/> <em>Alex Zhuang, Ge Zhang, Tianyu Zheng, Xeron Du, Junjie Wang, Weiming Ren, Wenhao Huang, Jie Fu, Xiang Yue, Wenhu Chen</em></p>
 

<p>D2PO: Discriminator-Guided DPO with Response Evaluation Models <br/> <em>Prasann Singhal, Nathan Lambert, Scott Niekum, Tanya Goyal, Greg Durrett</em></p>
 

<p>Tower: An Open Multilingual Large Language Model for Translation-Related Tasks <br/> <em>Duarte Miguel Alves, Jos√© Pombal, Nuno M Guerreiro, Pedro Henrique Martins, Jo√£o Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, Jos√© G. C. de Souza, Andre Martins</em></p>
 

<p>Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models <br/> <em>Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, Yinfei Yang</em></p>
 

<p>Self-Guide: Better Task-Specific Instruction Following via Self-Synthetic Finetuning <br/> <em>Chenyang Zhao, Xueying Jia, Vijay Viswanathan, Graham Neubig, Tongshuang Wu</em></p>
 

<p>3M-Diffusion: Latent Multi-Modal Diffusion for Language-Guided Molecule Generation <br/> <em>Huaisheng Zhu, Teng Xiao, Vasant G Honavar</em></p>
 

<p>CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting <br/> <em>Huihan Li, Liwei Jiang, Nouha Dziri, Xiang Ren, Yejin Choi</em></p>
 

<p>LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models <br/> <em>Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, Huaxiu Yao</em></p>
 

<p>CTIKG: LLM-Powered Knowledge Graph Construction from Cyber Threat Intelligence <br/> <em>Liangyi Huang, Xusheng Xiao</em></p>
 

<p>Enhancing Adversarial Robustness of LLMs with Analytic Hierarchy Process <br/> <em>Jiahao Zhao, Minzheng Wang, Nan Xu, YinLuo, Wenji Mao</em></p>
 

<p>Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions <br/> <em>Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Jacob Ginesin, Edward Berman, George Chakhnashvili, Anton Lozhkov, Carolyn Jane Anderson, Arjun Guha</em></p>
 

<p>Length-Controlled AlpacaEval: A Simple Debiasing of Automatic Evaluators <br/> <em>Yann Dubois, Percy Liang, Tatsunori Hashimoto</em></p>
 

<p>STaR-GATE: Teaching Language Models to Ask Clarifying Questions <br/> <em>Chinmaya Andukuri, Jan-Philipp Fr√§nken, Tobias Gerstenberg, Noah Goodman</em></p>
 

<p>Should We Attend More or Less? Modulating Attention for Fairness <br/> <em>Abdelrahman Zayed, Goncalo Mordido, Samira Shabanian, Sarath Chandar</em></p>
 

<p>On Robustness-Accuracy Characterization of Language Models using Synthetic Datasets <br/> <em>Ching-Yun Ko, Pin-Yu Chen, Payel Das, Yung-Sung Chuang, Luca Daniel</em></p>
 

<p>Handling Open-Vocabulary Constructs in Formalizing Specifications: Retrieval Augmented Parsing with Expert Knowledge <br/> <em>Mohammad Saqib Hasan, Sayontan Ghosh, Dhruv Verma, Geoff Kuenning, Erez Zadok, Scott Smolka, Niranjan Balasubramanian</em></p>
 

<p>Do Language Models Plan Ahead for Future Tokens? <br/> <em>Wilson Wu, John Xavier Morris, Lionel Levine</em></p>
 

<p>Automata-based constraints for language model decoding <br/> <em>Terry Koo, Frederick Liu, Luheng He</em></p>
 

<p>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversations <br/> <em>Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang</em></p>
 

<p>TOFU: A Task of Fictitious Unlearning for LLMs <br/> <em>Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary Chase Lipton, J Zico Kolter</em></p>
 

<p>SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design <br/> <em>Carl Edwards, Aakanksha Naik, Tushar Khot, Martin D. Burke, Heng Ji, Tom Hope</em></p>
 

<p>Inspecting and Editing Knowledge Representations in Language Models <br/> <em>Evan Hernandez, Belinda Z. Li, Jacob Andreas</em></p>
 

<p>Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators <br/> <em>Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vuliƒá, Anna Korhonen, Nigel Collier</em></p>
 

<p>CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs <br/> <em>Jingzhe Shi, Jialuo Li, Qinwei Ma, Zaiwen Yang, Huan Ma, Lei Li</em></p>
 

<p>Forcing Diffuse Distributions out of Language Models <br/> <em>Yiming Zhang, Avi Schwarzschild, Nicholas Carlini, J Zico Kolter, Daphne Ippolito</em></p>
 

<p>Certifying LLM Safety against Adversarial Prompting <br/> <em>Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, Himabindu Lakkaraju</em></p>
 

<p>Latent Causal Probing: A Formal Perspective on Probing with Causal Models of Data <br/> <em>Charles Jin</em></p>
 

<p>TMMLU+: An Improved Traditional Chinese Evaluation Suite for Foundation Models <br/> <em>Zhi Rui Tam, Ya Ting Pai, Yen-Wei Lee, Hong-Han Shuai, Jun-Da Chen, Wei Min Chu, Sega Cheng</em></p>
 

<p>BumbleBee: Dynamic KV Cache Summarization in Transformers using Submodular Optimization <br/> <em>Lilly Kumari, Shengjie Wang, Tianyi Zhou, Nikhil Sarda, Anthony Rowe, Jeff Bilmes</em></p>
 

<p>Keep the Cost Down: A Review on Methods to Optimize LLM‚Äôs KV-Cache Consumption <br/> <em>Shi Luohe, Hongyi Zhang, Yao Yao, Zuchao Li, hai zhao</em></p>
 

<p>PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models <br/> <em>Siddharth Mishra-Sharma, YIDING SONG, Jesse Thaler</em></p>
 

<p>IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models <br/> <em>Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yue Dong, Rifat Shahriyar</em></p>
 

<p>Yes, no, maybe? Revisiting language models&#39; response stability under paraphrasing for the assessment of political leaning <br/> <em>Patrick Haller, Jannis Vamvas, Lena Ann J√§ger</em></p>
 

<p>Measuring Taiwanese Mandarin Language Understanding <br/> <em>Po-Heng Chen, Sijia Cheng, Wei-Lin Chen, Yen-Ting Lin, Yun-Nung Chen</em></p>
 

<p>Pairwise Proximal Policy Optimization: Language Model Alignment with Comparative RL <br/> <em>Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, Jiantao Jiao</em></p>
 

<p>Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness <br/> <em>Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Tongshuang Wu</em></p>
 

<p>MouSi: Poly-Visual-Expert Vision-Language Models <br/> <em>Xiaoran Fan, Tao Ji, Ê±üÂ∏∏Áöì, Shuo Li, Senjie Jin, Sirui Song, Junke Wang, Boyang Hong, Lu Chen, Guodong Zheng, Ming Zhang, Caishuang Huang, Rui Zheng, Zhiheng Xi, Yuhao Zhou, Shihan Dou, Junjie Ye, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang</em></p>
 

<p>Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration <br/> <em>Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, Lingpeng Kong</em></p>
 

<p>MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models <br/> <em>Peng Ding, Jiading Fang, Peng Li, Kangrui Wang, Xiaochen Zhou, Mo Yu, Jing Li, Hongyuan Mei, Matthew Walter</em></p>
 

<p>ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning <br/> <em>Yuxuan Wang, Alan Yuille, Zhuowan Li, Zilong Zheng</em></p>
 

<p>Measuring and Controlling Instruction (In)Stability in Language Model Dialogs <br/> <em>Kenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Vi√©gas, Hanspeter Pfister, Martin Wattenberg</em></p>
 

<p>Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking <br/> <em>Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alexander D&#39;Amour, Krishnamurthy Dj Dvijotham, Adam Fisch, Katherine A Heller, Stephen R Pfohl, Deepak Ramachandran, Peter Shaw, Jonathan Berant</em></p>
 

<p>SteP: Stacked LLM Policies for Web Actions <br/> <em>Paloma Sodhi, S.R.K Branavan, Yoav Artzi, Ryan McDonald</em></p>
 

<p>LLM-Datasets: An Open Framework for Pretraining Datasets of Large Language Models <br/> <em>Malte Ostendorff, Pedro Ortiz Suarez, Lucas Fonseca Lage, Georg Rehm</em></p>
 

<p>Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization <br/> <em>Costas Mavromatis, Petros Karypis, George Karypis</em></p>
 

<p>Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners <br/> <em>Jihyeon Lee, Dain Kim, Doohae Jung, Boseop Kim, Kyoung-Woon On</em></p>
 

<p>Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data <br/> <em>Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Tomasz Korbak, Henry Sleight, Rajashree Agrawal, John Hughes, Dhruv Bhandarkar Pai, Andrey Gromov, Dan Roberts, Diyi Yang, David L. Donoho, Sanmi Koyejo</em></p>
 

<p>Prompt-prompted Mixture of Experts for Efficient LLM Generation <br/> <em>Harry Dong, Beidi Chen, Yuejie Chi</em></p>
 

<p>WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting <br/> <em>Olly Styles, Sam Miller, Patricio Cerda-Mardini, Tanaya Guha, Victor Sanchez, Bertie Vidgen</em></p>
 

<p>Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation <br/> <em>Eric Zelikman, Eliana Lorch, Lester Mackey, Adam Tauman Kalai</em></p>
 

<p>Cohesive Conversations: Enhancing Authenticity in Multi-Agent Simulated Dialogues <br/> <em>KuanChao Chu, Yi-Pei Chen, Hideki Nakayama</em></p>
 

<p>StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows <br/> <em>Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, Qingyun Wu</em></p>
 

<p>MiniCPM: Unveiling the Potential of Small Language Models <br/> <em>Shengding Hu, Xu Han, Yuge Tu, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai li, Zhiyuan Liu, Maosong Sun</em></p>
 

<p>Timo: Towards Better Temporal Reasoning for Language Models <br/> <em>Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min zhang, Yu Cheng</em></p>
 

<p>Bot or Human? Detecting ChatGPT Imposters with A Single Question <br/> <em>Hong Wang, Xuan Luo, Weizhi Wang, Melody Yu, Xifeng Yan</em></p>
 

<p>Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs <br/> <em>Pengda Wang, Zilin Xiao, Hanjie Chen, Frederick L. Oswald</em></p>
 

<p>Enhancing Language Models with Idiomatic Reasoning <br/> <em>Jianing Zhou, Ziheng Zeng, Hongyu Gong, Suma Bhat</em></p>
 

<p>ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation <br/> <em>Ana Brassard, Benjamin Heinzerling, Keito Kudo, Keisuke Sakaguchi, Kentaro Inui</em></p>
 

<p>ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction <br/> <em>Mingyu Jin, Haochen Xue, Zhenting Wang, Boming Kang, Ruosong Ye, Kaixiong Zhou, Mengnan Du, Yongfeng Zhang</em></p>
 

<p>Stream of Search (SoS): Learning to Search in Language <br/> <em>Kanishk Gandhi, Denise H J Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, Noah Goodman</em></p>
 

<p>Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation <br/> <em>Declan Grabb, Max Lamparth, Nina Vasan</em></p>
 

<p>Prompt Public Large Language Models to Synthesize Data for Private On-device Applications <br/> <em>Shanshan Wu, Zheng Xu, Yanxiang Zhang, Yuanbo Zhang, Daniel Ramage</em></p>
 

<p>Agent-DocEdit: Language-Instructed LLM Agent for Content-Rich Document Editing <br/> <em>Te-Lin Wu, Rajiv Jain, Yufan Zhou, Puneet Mathur, Vlad I Morariu</em></p>
 

<p>From Strategic Narratives to Code-Like Cognitive Models: An LLM-Based Approach in A Sorting Task <br/> <em>Hanbo Xie, Hua-Dong Xiong, Robert Wilson</em></p>
 

<p>See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses <br/> <em>Yulong Chen, Jianhao Yan, Xuefeng Bai, Ming Zhong, Yinghao Yang, Ziyi Yang, Chenguang Zhu, Yang Liu, Yue Zhang</em></p>
 

<p>SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval <br/> <em>Griffin Thomas Adams, Jason Zucker, No√©mie Elhadad</em></p>
 

<p>Effective Prompt Extraction from Language Models <br/> <em>Yiming Zhang, Nicholas Carlini, Daphne Ippolito</em></p>
 

<p>ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for Contrastive Self-Training <br/> <em>Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</em></p>
 

<p>InstructAV: Instruction Fine-tuning Large Language Models for Authorship Verification <br/> <em>Yujia Hu, Zhiqiang Hu, Chun Wei Seah, Roy Ka-Wei Lee</em></p>
 




    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">¬© 2024 COLM Organization Committee</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>