UID,image,title,institution,speaker,abstract,bio,session,url
1,/static/images/plenary/luke_zettlemoyer.png,"Mixed-modal Language Modeling","University of Washington / Meta",Luke Zettlemoyer,"Multimodal architectures are typically designed for specific modalities (image->text, text->image, text only, etc). In this talk, I will present our recent work on a series of early fusion mixed-modal models with generalized architectures that can instead generate arbitrary mixed sequences of images and text. Such models have the ability to unlock fundamentally new multimodal chain-of-thought reasoning capabilities, as I will show through an early model for multimodal tool use, but determining the best mixed-model architecture remains an open challenge. I will discuss and contrast two models architectures, Chameleon and Transfusion, that make very different assumptions about how to model mixed-modal data, and argue for moving from a tokenize-everything approach to newer models that are hybrids of autoregressive transformers and diffusion. I will also cover recent efforts to better understand how to more stably train such models at scale without excessive modality competition, using a mixture of transformers technique. Together, these advances lay a possible foundation for universal models that can understand and generate data in any modality, and I will also sketch some of the steps that we still need to focus on to reach this goal.","Luke Zettlemoyer is a Professor in the Paul G. Allen School of Computer Science & Engineering at the University of Washington, and a Senior Research Director at Meta. His research focuses on empirical methods for natural language semantics, and involves designing machine learning algorithms, introducing new tasks and datasets, and, most recently, studying how to best develop self-supervision signals for pre-training. His honors include being elected ACL President, named an ACL Fellow, winning a PECASE award, an Allen Distinguished Investigator award, and multiple best paper awards. Luke was an undergrad at NC State University, received his PhD from MIT and was a postdoc at the University of Edinburgh.",TDB,https://homes.cs.washington.edu/~lsz/
2,/static/images/plenary/tom_griffiths.jpg,"Mapping the Jagged Edges of AI with Cognitive Science","Princeton University",Tom Griffiths,"Current artificial intelligence systems demonstrate surprising amount of heterogeneity in their abilities, displaying superhuman competence in some tasks but puzzling limitations in others. I will argue that the tools we need for understanding this heterogeneity can be found in cognitive science, where researchers have spent decades developing theoretical and empirical methods for making sense of the capabilities of intelligent systems. Work by cognitive scientists suggests two strategies for mapping the jagged edges of AI: identifying general properties of neural networks that might translate into limitations for current AI systems, and considering cases where human minds might provide a guide to problems that pose a challenge for AI. I will present examples of both strategies, discussing some surprising cases where large language models perform poorly in predictable ways and recent results using the limits of human cognition to predict cases where large language models and vision language models fail.","TBD",TDB,https://cocosci.princeton.edu/tom/index.php
3,/static/images/plenary/nicholas_carlini.jpg,"Are the harms and risks of LLMs worth it?","Anthropic",Nicholas Carlini,"Having largely succeed at creating highly effective language models over the past decades, this talk examines the risks we now face. I discuss both the immediate harms that we are already facing today (e.g., sycophancy), risks that seem inevitable to arrive in short order (e.g., LLM aided abuse), and 'existential risks' some argue are likely to arrive soon after that (e.g., Doom?). I argue that each of these risks fall on a single continuum, and progress mitigating any one of these risks contributes positively towards mitigating the others. I propose several research directions that have been under-explored by the academic ML/LLM research community that could help mitigate these risks, and advocate for increased collaboration between all those who are concerned about risks of 'AI' broadly construed.","Nicholas Carlini is a research scientist at Anthropic working at the intersection of security and machine learning. His current work studies what harms an adversary could do with, or do to, language models. For his work, he has received best paper award from ICML, EuroCrypt, USENIX Security, and IEEE S&P. He obtained his PhD from UC Berkeley in 2018 under David Wagner.",TDB,https://nicholas.carlini.com/
4,/static/images/plenary/Shirley_Ho.jpg,"Building a Polymathic Foundation Model for Science","Flatiron Institute",Shirley Ho,"TBD","TBD",TDB,https://www.shirleyho.space/
6,/static/images/plenary/Gillian_Hadfield.jpg,Keynote,"Johns Hopkins University",Gillian Hadfield,"TBD","Gillian K. Hadfield is the Bloomberg Distinguished Professor of AI Alignment and Governance at the Whiting School of Engineering and the School of Government and Policy at Johns Hopkins University. She is a faculty member of the Vector Institute for Artificial Intelligence and is a Schmidt Sciences AI2050 Senior Fellow. Originally trained as an economist and legal scholar, Hadfieldâ€™s research now focuses on innovative design for legal, regulatory, and technical systems for AI, computational models of human normative systems, and building AI systems that understand and respond to human values and norms.",TDB,https://gillianhadfield.org/